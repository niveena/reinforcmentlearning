{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e653564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Niveen Abdul-Mohsen (bvn9ad)\n",
    "# Reinforcement Learning (CS 4771) - Fgure 5.1\n",
    "# Monte Carlo On policy\n",
    "# i used numpy for numerical operations and matplotlib for plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import defaultdict\n",
    "\n",
    "class BlackjackEnvironment:\n",
    "    \"\"\"\n",
    "    Blackjack Environment Implementation\n",
    "    \n",
    "    From Sutton & Barto Section 5.1, Example 5.1:\n",
    "    - The object of blackjack is to obtain cards whose sum is as great as \n",
    "      possible without exceeding 21\n",
    "    - All face cards count as 10, and an ace can count either as 1 or as 11\n",
    "    - Each player competes independently against the dealer\n",
    "    - The game begins with two cards dealt to both dealer and player\n",
    "    - One of the dealer's cards is face up (showing card) and one is face down\n",
    "    \n",
    "    State Space (200 states total):\n",
    "    - Player's current sum (12-21): 10 possible values\n",
    "    - Dealer's showing card (Ace-10): 10 possible values  \n",
    "    - Whether player holds a usable ace (True/False): 2 possible values\n",
    "    - Total: 10 × 10 × 2 = 200 states\n",
    "    \n",
    "    Note: Player sums below 12 are not included because the player should \n",
    "    always hit with sum < 12 (no decision to be made)\n",
    "    \n",
    "    Reward Structure:\n",
    "    - +1 for winning\n",
    "    - -1 for losing\n",
    "    - 0 for draw\n",
    "    - All intermediate rewards are 0\n",
    "    - Discount factor γ = 1 (no discounting) as stated in textbook\n",
    "    \n",
    "    Environment Assumptions (from textbook):\n",
    "    - Cards are dealt from an infinite deck (with replacement)\n",
    "    - No advantage to keeping track of cards already dealt\n",
    "    \"\"\"\n",
    "    \n",
    "    # Action constants for readability\n",
    "    ACTION_STICK = 0\n",
    "    ACTION_HIT = 1\n",
    "    \n",
    "    # Dealer threshold - dealer must stick at this sum or higher\n",
    "    DEALER_STICK_THRESHOLD = 17\n",
    "    \n",
    "    # Blackjack winning total\n",
    "    BLACKJACK_TARGET = 21\n",
    "    \n",
    "    # Reward constants\n",
    "    REWARD_WIN = 1\n",
    "    REWARD_LOSS = -1\n",
    "    REWARD_DRAW = 0\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the blackjack environment\n",
    "        \n",
    "        For infinite deck assumption, we set up uniform probabilities\n",
    "        for cards 1-10, where:\n",
    "        - Card 1 represents Ace\n",
    "        - Cards 2-9 represent their face value\n",
    "        - Card 10 represents 10, J, Q, K (all valued at 10)\n",
    "        \"\"\"\n",
    "        # Current game state\n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "        \n",
    "        # Random number generator for reproducibility if needed\n",
    "        self.rng = np.random.default_rng()\n",
    "    \n",
    "    def draw_card(self):\n",
    "        \"\"\"\n",
    "        Draw a card from infinite deck\n",
    "        \n",
    "        Returns:\n",
    "            int: Card value (1-10, where 1 is Ace)\n",
    "        \n",
    "        Note from textbook: \n",
    "        - Cards are dealt with replacement (infinite deck assumption)\n",
    "        - This simplifies the problem by making draws independent\n",
    "        \n",
    "        Implementation details:\n",
    "        - We use discrete uniform distribution over [1, 10]\n",
    "        - Card value 10 represents any 10-value card (10, J, Q, K)\n",
    "        - This matches the textbook's simplified model\n",
    "        \"\"\"\n",
    "        # Draw uniformly from 1 to 10 (inclusive)\n",
    "        # In a real deck, 10s are more common, but textbook uses uniform\n",
    "        card_value = self.rng.integers(low=1, high=11)\n",
    "        return card_value\n",
    "    \n",
    "    def usable_ace(self, hand):\n",
    "        \"\"\"\n",
    "        Determine if the hand has a usable ace\n",
    "        \n",
    "        From textbook Section 5.1:\n",
    "        \"If the player holds an ace that he could count as 11 without going bust,\n",
    "        then the ace is said to be usable. In this case it is always counted as 11\n",
    "        because counting it as 1 would make the sum 11 or less, in which case there\n",
    "        is no decision to be made because, obviously, the player should always hit.\"\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if hand has usable ace, False otherwise\n",
    "            \n",
    "        Logic:\n",
    "        - An ace is \"usable\" if it can be counted as 11 without busting\n",
    "        - This means: (sum of all cards) + 10 <= 21\n",
    "        - The +10 comes from counting one ace as 11 instead of 1\n",
    "        \"\"\"\n",
    "        # Check if hand contains at least one ace (value 1)\n",
    "        has_ace = 1 in hand\n",
    "        \n",
    "        if not has_ace:\n",
    "            return False\n",
    "        \n",
    "        # Calculate sum with all aces counted as 1\n",
    "        current_sum = sum(hand)\n",
    "        \n",
    "        # Check if we can count one ace as 11 (adding 10 to current sum)\n",
    "        # without exceeding 21\n",
    "        can_use_ace_as_eleven = (current_sum + 10) <= self.BLACKJACK_TARGET\n",
    "        \n",
    "        return can_use_ace_as_eleven\n",
    "    \n",
    "    def sum_hand(self, hand):\n",
    "        \"\"\"\n",
    "        Calculate the sum of a hand, accounting for usable ace\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            int: Sum of hand (with ace counted optimally)\n",
    "            \n",
    "        Implementation note:\n",
    "        - Start by counting all aces as 1\n",
    "        - If there's a usable ace, add 10 to count it as 11\n",
    "        - This gives us the optimal sum without busting\n",
    "        \"\"\"\n",
    "        # Sum all cards (aces counted as 1)\n",
    "        total_sum = sum(hand)\n",
    "        \n",
    "        # If we have a usable ace, count it as 11 instead of 1\n",
    "        # This adds 10 to the total\n",
    "        if self.usable_ace(hand):\n",
    "            total_sum += 10\n",
    "        \n",
    "        return total_sum\n",
    "    \n",
    "    def is_bust(self, hand):\n",
    "        \"\"\"\n",
    "        Check if hand is bust (sum > 21)\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if bust, False otherwise\n",
    "        \"\"\"\n",
    "        return self.sum_hand(hand) > self.BLACKJACK_TARGET\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Start a new episode\n",
    "        \n",
    "        From textbook:\n",
    "        \"The game begins with two cards dealt to both dealer and player.\n",
    "        One of the dealer's cards is face up and the other is face down.\"\n",
    "        \n",
    "        Returns:\n",
    "            state: Tuple of (player_sum, dealer_showing, usable_ace)\n",
    "            \n",
    "        Implementation notes:\n",
    "        - Deal initial cards to both player and dealer\n",
    "        - If player gets natural (21), handle it according to rules\n",
    "        - Return the initial observable state\n",
    "        \"\"\"\n",
    "        # Deal two cards to player\n",
    "        self.player_hand = [self.draw_card(), self.draw_card()]\n",
    "        \n",
    "        # Deal two cards to dealer\n",
    "        # First card is \"showing\" (face up), second is \"hole card\" (face down)\n",
    "        self.dealer_hand = [self.draw_card(), self.draw_card()]\n",
    "        \n",
    "        # Extract dealer's showing card (first card by convention)\n",
    "        dealer_showing_card = self.dealer_hand[0]\n",
    "        \n",
    "        # Calculate player's sum and check for usable ace\n",
    "        player_total = self.sum_hand(self.player_hand)\n",
    "        has_usable_ace = self.usable_ace(self.player_hand)\n",
    "        \n",
    "        # Create state tuple: (player sum, dealer showing, usable ace)\n",
    "        initial_state = (player_total, dealer_showing_card, has_usable_ace)\n",
    "        \n",
    "        return initial_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment\n",
    "        \n",
    "        Args:\n",
    "            action: 0 = stick, 1 = hit\n",
    "            \n",
    "        Returns:\n",
    "            next_state: Next state after action (or None if terminal)\n",
    "            reward: Reward received\n",
    "            done: Whether episode is complete\n",
    "            \n",
    "        From textbook Section 5.1:\n",
    "        \"The player's actions are to hit or to stick.\"\n",
    "        \n",
    "        Implementation structure:\n",
    "        - If player hits: draw card, check for bust, return new state\n",
    "        - If player sticks: dealer plays, determine winner, return reward\n",
    "        \"\"\"\n",
    "        # CASE 1: Player chooses to HIT\n",
    "        if action == self.ACTION_HIT:\n",
    "            # Draw a new card and add to player's hand\n",
    "            new_card = self.draw_card()\n",
    "            self.player_hand.append(new_card)\n",
    "            \n",
    "            # Check if player went bust\n",
    "            if self.is_bust(self.player_hand):\n",
    "                # Player loses immediately\n",
    "                # Episode ends, no next state needed\n",
    "                return None, self.REWARD_LOSS, True\n",
    "            \n",
    "            # Player didn't bust - game continues\n",
    "            # Calculate new state\n",
    "            player_total = self.sum_hand(self.player_hand)\n",
    "            dealer_showing_card = self.dealer_hand[0]\n",
    "            has_usable_ace = self.usable_ace(self.player_hand)\n",
    "            \n",
    "            next_state = (player_total, dealer_showing_card, has_usable_ace)\n",
    "            \n",
    "            # No reward yet (intermediate reward is 0)\n",
    "            return next_state, 0, False\n",
    "        \n",
    "        # CASE 2: Player chooses to STICK\n",
    "        elif action == self.ACTION_STICK:\n",
    "            # Player's turn is over - now dealer plays\n",
    "            dealer_final_sum, dealer_busted = self.dealer_policy()\n",
    "            \n",
    "            # Calculate player's final sum\n",
    "            player_final_sum = self.sum_hand(self.player_hand)\n",
    "            \n",
    "            # Determine outcome and assign reward\n",
    "            if dealer_busted:\n",
    "                # Dealer went bust - player wins!\n",
    "                reward = self.REWARD_WIN\n",
    "            elif player_final_sum > dealer_final_sum:\n",
    "                # Player has higher sum - player wins!\n",
    "                reward = self.REWARD_WIN\n",
    "            elif player_final_sum < dealer_final_sum:\n",
    "                # Dealer has higher sum - player loses\n",
    "                reward = self.REWARD_LOSS\n",
    "            else:\n",
    "                # Same sum - it's a draw\n",
    "                reward = self.REWARD_DRAW\n",
    "            \n",
    "            # Episode is complete\n",
    "            return None, reward, True\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}. Must be 0 (stick) or 1 (hit)\")\n",
    "    \n",
    "    def dealer_policy(self):\n",
    "        \"\"\"\n",
    "        Dealer's fixed strategy\n",
    "        \n",
    "        From textbook Section 5.1:\n",
    "        \"The dealer hits or sticks according to a fixed strategy without choice:\n",
    "        he sticks on any sum of 17 or greater, and hits otherwise.\"\n",
    "        \n",
    "        Returns:\n",
    "            final_sum: Dealer's final hand sum\n",
    "            is_bust: Whether dealer went bust\n",
    "            \n",
    "        Implementation:\n",
    "        - Dealer keeps hitting until reaching threshold or busting\n",
    "        - No choice involved - purely deterministic based on current sum\n",
    "        \"\"\"\n",
    "        # Dealer plays out their hand\n",
    "        while True:\n",
    "            current_dealer_sum = self.sum_hand(self.dealer_hand)\n",
    "            \n",
    "            # Check if dealer should stick\n",
    "            if current_dealer_sum >= self.DEALER_STICK_THRESHOLD:\n",
    "                # Dealer sticks - return final sum and bust status\n",
    "                dealer_busted = current_dealer_sum > self.BLACKJACK_TARGET\n",
    "                return current_dealer_sum, dealer_busted\n",
    "            \n",
    "            # Dealer must hit (sum < 17)\n",
    "            new_card = self.draw_card()\n",
    "            self.dealer_hand.append(new_card)\n",
    "            \n",
    "            # Check if dealer busted after drawing\n",
    "            if self.is_bust(self.dealer_hand):\n",
    "                # Dealer went bust\n",
    "                final_sum = self.sum_hand(self.dealer_hand)\n",
    "                return final_sum, True\n",
    "    \n",
    "    def natural(self, hand):\n",
    "        \"\"\"\n",
    "        Check if hand is a natural (Ace + 10-card = 21 with 2 cards)\n",
    "        \n",
    "        From textbook:\n",
    "        \"If the player has 21 immediately (an ace and a 10-card), it is called \n",
    "        a natural. He then wins unless the dealer also has a natural, in which \n",
    "        case the game is a draw.\"\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if natural, False otherwise\n",
    "            \n",
    "        Logic:\n",
    "        - Must have exactly 2 cards\n",
    "        - Must sum to exactly 21\n",
    "        - Must contain an ace (value 1) and a 10-value card\n",
    "        \"\"\"\n",
    "        # Check for exactly 2 cards\n",
    "        if len(hand) != 2:\n",
    "            return False\n",
    "        \n",
    "        # Check if sum equals 21\n",
    "        # (use sum_hand to properly handle ace)\n",
    "        if self.sum_hand(hand) != self.BLACKJACK_TARGET:\n",
    "            return False\n",
    "        \n",
    "        # Check for ace and 10-card combination\n",
    "        # A natural requires one ace (1) and one 10-value card (10)\n",
    "        has_ace = 1 in hand\n",
    "        has_ten = 10 in hand\n",
    "        \n",
    "        return has_ace and has_ten\n",
    "\n",
    "\n",
    "def simple_policy(state):\n",
    "    \"\"\"\n",
    "    The policy to evaluate (from Figure 5.1 caption)\n",
    "    \n",
    "    From textbook Section 5.1:\n",
    "    \"Consider the policy that sticks if the player's sum is 20 or 21, \n",
    "    and otherwise hits.\"\n",
    "    \n",
    "    This is a fixed, deterministic policy:\n",
    "    - Stick (action 0) if player sum is 20 or 21\n",
    "    - Hit (action 1) otherwise\n",
    "    \n",
    "    Args:\n",
    "        state: Tuple of (player_sum, dealer_showing, usable_ace)\n",
    "        \n",
    "    Returns:\n",
    "        action: 0 = stick, 1 = hit\n",
    "    \"\"\"\n",
    "    # Unpack state tuple\n",
    "    player_sum, dealer_showing, usable_ace = state\n",
    "    \n",
    "    # Policy decision based on player's sum\n",
    "    if player_sum >= 20:\n",
    "        # Stick on 20 or 21\n",
    "        return BlackjackEnvironment.ACTION_STICK\n",
    "    else:\n",
    "        # Hit on anything less than 20\n",
    "        return BlackjackEnvironment.ACTION_HIT\n",
    "\n",
    "\n",
    "# Simple test to verify environment works correctly\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing Blackjack Environment Implementation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create environment\n",
    "    env = BlackjackEnvironment()\n",
    "    \n",
    "    # Test 1: Play a few episodes with the simple policy\n",
    "    print(\"\\nTest 1: Playing 5 episodes with simple policy (stick on 20+)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for episode_num in range(5):\n",
    "        print(f\"\\nEpisode {episode_num + 1}:\")\n",
    "        \n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        print(f\"  Initial state: {state}\")\n",
    "        print(f\"  Player hand: {env.player_hand} (sum: {env.sum_hand(env.player_hand)})\")\n",
    "        print(f\"  Dealer showing: {env.dealer_hand[0]}\")\n",
    "        \n",
    "        # Play episode following simple policy\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from policy\n",
    "            action = simple_policy(state)\n",
    "            action_name = \"STICK\" if action == 0 else \"HIT\"\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            print(f\"  Step {step_count + 1}: Action = {action_name}\")\n",
    "            \n",
    "            if not done:\n",
    "                print(f\"    New state: {next_state}\")\n",
    "                print(f\"    Player hand: {env.player_hand} (sum: {env.sum_hand(env.player_hand)})\")\n",
    "                state = next_state\n",
    "            else:\n",
    "                print(f\"    Episode ended!\")\n",
    "                print(f\"    Final player hand: {env.player_hand} (sum: {env.sum_hand(env.player_hand)})\")\n",
    "                print(f\"    Final dealer hand: {env.dealer_hand} (sum: {env.sum_hand(env.dealer_hand)})\")\n",
    "                print(f\"    Reward: {reward}\")\n",
    "                \n",
    "                if reward == 1:\n",
    "                    print(f\"    Result: PLAYER WINS!\")\n",
    "                elif reward == -1:\n",
    "                    print(f\"    Result: PLAYER LOSES\")\n",
    "                else:\n",
    "                    print(f\"    Result: DRAW\")\n",
    "            \n",
    "            step_count += 1\n",
    "    \n",
    "    # Test 2: Test helper functions\n",
    "    print(\"\\n\\nTest 2: Testing helper functions\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Test usable ace\n",
    "    test_hands = [\n",
    "        [1, 5],      # Ace-5: usable ace (can count as 11 for 16)\n",
    "        [1, 10],     # Ace-10: usable ace (natural 21)\n",
    "        [1, 9, 5],   # Ace-9-5: no usable ace (would be 25 if ace=11)\n",
    "        [10, 7],     # 10-7: no ace at all\n",
    "        [1, 1, 9],   # Ace-Ace-9: usable ace (one counts as 11)\n",
    "    ]\n",
    "    \n",
    "    for hand in test_hands:\n",
    "        total = env.sum_hand(hand)\n",
    "        usable = env.usable_ace(hand)\n",
    "        is_natural = env.natural(hand)\n",
    "        print(f\"  Hand {hand}: sum={total}, usable_ace={usable}, natural={is_natural}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Environment tests complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def first_visit_mc_prediction(policy, env, num_episodes, gamma=1.0):\n",
    "    \"\"\"\n",
    "    First-visit Monte Carlo policy evaluation\n",
    "    \n",
    "    Algorithm from Sutton & Barto Section 5.1 (page 92):\n",
    "    \n",
    "    Input: a policy π to be evaluated\n",
    "    Initialize:\n",
    "        V(s) ∈ ℝ, arbitrarily, for all s ∈ S\n",
    "        Returns(s) ← an empty list, for all s ∈ S\n",
    "    Loop forever (for each episode):\n",
    "        Generate an episode following π: S₀, A₀, R₁, S₁, A₁, R₂, ..., S_{T-1}, A_{T-1}, R_T\n",
    "        G ← 0\n",
    "        Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
    "            G ← γG + R_{t+1}\n",
    "            Unless S_t appears in S₀, S₁, ..., S_{t-1}:\n",
    "                Append G to Returns(S_t)\n",
    "                V(S_t) ← average(Returns(S_t))\n",
    "    \n",
    "    Key concepts from textbook:\n",
    "    - \"First-visit MC method estimates v_π(s) as the average of the returns \n",
    "       following first visits to s\"\n",
    "    - \"Each return is an independent, identically distributed estimate of v_π(s)\"\n",
    "    - \"By the law of large numbers the sequence of averages converges to the \n",
    "       expected value\"\n",
    "    - \"The standard deviation of error falls as 1/√n where n is the number of \n",
    "       returns averaged\"\n",
    "    \n",
    "    Args:\n",
    "        policy: Function mapping states to actions\n",
    "        env: BlackjackEnvironment instance\n",
    "        num_episodes: Number of episodes to run (10,000 or 500,000 for Figure 5.1)\n",
    "        gamma: Discount factor (1.0 for blackjack as per textbook)\n",
    "        \n",
    "    Returns:\n",
    "        V: Dictionary mapping states to estimated values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize value function arbitrarily\n",
    "    # V maps state -> estimated value\n",
    "    # TODO: Initialize V as defaultdict(float) or nested dict\n",
    "    V = None\n",
    "    \n",
    "    # Returns(s) stores list of returns following first visits to state s\n",
    "    # From textbook: We need to track all returns to compute the average\n",
    "    # TODO: Initialize Returns as defaultdict(list) or nested dict\n",
    "    Returns = None\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode_num in range(num_episodes):\n",
    "        \n",
    "        # GENERATE AN EPISODE following policy π\n",
    "        # Episode is sequence: S₀, A₀, R₁, S₁, A₁, R₂, ..., S_{T-1}, A_{T-1}, R_T\n",
    "        # TODO: Initialize episode list to store (state, action, reward) tuples\n",
    "        episode = []\n",
    "        \n",
    "        # TODO: Reset environment to get initial state S₀\n",
    "        state = None\n",
    "        \n",
    "        # TODO: Generate episode until terminal state\n",
    "        # Loop:\n",
    "        #   - Get action from policy\n",
    "        #   - Take step in environment\n",
    "        #   - Store (state, action, reward) in episode\n",
    "        #   - Update state\n",
    "        #   - Break if done\n",
    "        \n",
    "        # CALCULATE RETURNS AND UPDATE VALUE FUNCTION\n",
    "        # Work backwards from end of episode\n",
    "        # G is the return (cumulative discounted reward)\n",
    "        G = 0\n",
    "        \n",
    "        # TODO: Loop backwards through episode (t = T-1, T-2, ..., 0)\n",
    "        # for t in range(len(episode)-1, -1, -1):\n",
    "        \n",
    "            # TODO: Extract state and reward from episode[t]\n",
    "            \n",
    "            # Calculate return: G ← γG + R_{t+1}\n",
    "            # From textbook: For blackjack, γ = 1, so G ← G + R_{t+1}\n",
    "            # TODO: Update G\n",
    "            \n",
    "            # FIRST-VISIT CHECK\n",
    "            # \"Unless S_t appears in S₀, S₁, ..., S_{t-1}\"\n",
    "            # This is the key difference between first-visit and every-visit MC\n",
    "            # TODO: Check if current state appeared earlier in episode\n",
    "            # if state not in [episode[i] for i in range(t)]:\n",
    "            \n",
    "                # Append return to Returns(S_t)\n",
    "                # TODO: Append G to Returns[state]\n",
    "                \n",
    "                # Update value estimate: V(S_t) ← average(Returns(S_t))\n",
    "                # From textbook: \"The value estimate is the sample mean of returns\"\n",
    "                # TODO: V[state] = mean(Returns[state])\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def state_value_to_arrays(V, usable_ace):\n",
    "    \"\"\"\n",
    "    Convert state-value dictionary to 2D arrays for plotting\n",
    "    \n",
    "    The state space for plotting is:\n",
    "    - X-axis: Dealer showing card (1-10, representing Ace through 10)\n",
    "    - Y-axis: Player sum (12-21)\n",
    "    - Z-axis: Value V(s)\n",
    "    - Separate plots for usable ace vs. no usable ace\n",
    "    \n",
    "    Args:\n",
    "        V: Dictionary mapping (player_sum, dealer_showing, usable_ace) -> value\n",
    "        usable_ace: Boolean, which ace condition to extract\n",
    "        \n",
    "    Returns:\n",
    "        values: 2D numpy array of shape (10, 10) for plotting\n",
    "                rows = player sum (12-21)\n",
    "                cols = dealer showing (1-10)\n",
    "    \"\"\"\n",
    "    # TODO: Initialize 10x10 array (player sums 12-21, dealer showing 1-10)\n",
    "    # TODO: Extract values from V for given usable_ace condition\n",
    "    # TODO: Return 2D array suitable for surface plot\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_value_function(V, num_episodes, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create 3D surface plots matching Figure 5.1 layout\n",
    "    \n",
    "    Figure 5.1 shows:\n",
    "    - Left column: After 10,000 episodes\n",
    "    - Right column: After 500,000 episodes\n",
    "    - Top row: Usable ace\n",
    "    - Bottom row: No usable ace\n",
    "    \n",
    "    Each subplot shows:\n",
    "    - X-axis: Dealer showing (A, 2, 3, ..., 10)\n",
    "    - Y-axis: Player sum (12, 13, ..., 21)\n",
    "    - Z-axis: State value (approximately -1 to +1)\n",
    "    \n",
    "    The plots are wireframe/surface plots showing the value function as a 3D surface\n",
    "    \n",
    "    Args:\n",
    "        V: Value function dictionary\n",
    "        num_episodes: Number of episodes run (for title)\n",
    "        title_suffix: Additional text for title\n",
    "    \"\"\"\n",
    "    # TODO: Create figure with 2 subplots (usable ace, no usable ace)\n",
    "    \n",
    "    # TODO: For usable_ace in [True, False]:\n",
    "    #   - Convert V to 2D array\n",
    "    #   - Create meshgrid for X (dealer) and Y (player sum)\n",
    "    #   - Create 3D surface plot\n",
    "    #   - Set axis labels: \"Dealer showing\", \"Player sum\", value\n",
    "    #   - Set title: \"Usable ace\" or \"No usable ace\"\n",
    "    #   - Set z-axis limits approximately [-1, +1]\n",
    "    \n",
    "    # TODO: Add overall title: f\"After {num_episodes} episodes\"\n",
    "    # TODO: Adjust layout and show plot\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to recreate Figure 5.1\n",
    "    \n",
    "    From textbook Section 5.1:\n",
    "    \"In this way, we obtained the estimates of the state-value function shown \n",
    "    in Figure 5.1. The estimates for states with a usable ace are less certain \n",
    "    and less regular because these states are less common. In any event, after \n",
    "    500,000 games the value function is very well approximated.\"\n",
    "    \n",
    "    The figure shows results after:\n",
    "    1. 10,000 episodes (left column)\n",
    "    2. 500,000 episodes (right column)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Recreating Figure 5.1: Blackjack Monte Carlo Policy Evaluation\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize environment\n",
    "    # TODO: env = BlackjackEnvironment()\n",
    "    \n",
    "    # Run First-Visit MC for 10,000 episodes\n",
    "    print(\"Running First-Visit Monte Carlo Prediction...\")\n",
    "    print(\"Episodes: 10,000\")\n",
    "    # TODO: V_10k = first_visit_mc_prediction(simple_policy, env, 10000)\n",
    "    print(\"Completed 10,000 episodes\")\n",
    "    \n",
    "    # Plot results for 10,000 episodes\n",
    "    # TODO: plot_value_function(V_10k, 10000)\n",
    "    \n",
    "    # Run First-Visit MC for 500,000 episodes  \n",
    "    print(\"\\nEpisodes: 500,000\")\n",
    "    # TODO: V_500k = first_visit_mc_prediction(simple_policy, env, 500000)\n",
    "    print(\"Completed 500,000 episodes\")\n",
    "    \n",
    "    # Plot results for 500,000 episodes\n",
    "    # TODO: plot_value_function(V_500k, 500000)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO: main()\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
