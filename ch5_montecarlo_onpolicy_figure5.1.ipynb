{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e653564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing First-Visit Monte Carlo Prediction\n",
      "======================================================================\n",
      "\n",
      "Test 1: Running 1,000 episodes\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Sample state values (1,000 episodes):\n",
      "  (20, 10, True): 0.6667\n",
      "  (20, 5, False): 0.6000\n",
      "  (18, 10, True): -1.0000\n",
      "  (12, 10, False): -0.5263\n",
      "\n",
      "\n",
      "Test 2: Comparing two implementations (5,000 episodes)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Comparing implementations on sample states:\n",
      "  (12, 10, False):\n",
      "    List version:        -0.4286\n",
      "    Incremental version: -0.3108\n",
      "    Difference:          0.117761\n",
      "    Visit count:         74\n",
      "\n",
      "\n",
      "Test 3: Longer run to observe convergence\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Value of state (18, 10, False) over increasing episodes:\n",
      "    1000 episodes: -0.7333\n",
      "    5000 episodes: -0.3956\n",
      "Running First-Visit MC Prediction for 10000 episodes...\n",
      "  Completed 10000/10000 episodes\n",
      "First-Visit MC Prediction complete!\n",
      "Total unique states visited: 280\n",
      "   10000 episodes: -0.6497\n",
      "Running First-Visit MC Prediction for 20000 episodes...\n",
      "  Completed 10000/20000 episodes\n",
      "  Completed 20000/20000 episodes\n",
      "First-Visit MC Prediction complete!\n",
      "Total unique states visited: 280\n",
      "   20000 episodes: -0.6407\n"
     ]
    }
   ],
   "source": [
    "#Niveen Abdul-Mohsen (bvn9ad)\n",
    "# Reinforcement Learning (CS 4771) - Fgure 5.1\n",
    "# Monte Carlo On policy\n",
    "# i used numpy for numerical operations and matplotlib for plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import defaultdict\n",
    "\n",
    "class BlackjackEnvironment:\n",
    "    \"\"\"\n",
    "    Blackjack Environment Implementation\n",
    "    \n",
    "    From Sutton & Barto Section 5.1, Example 5.1:\n",
    "    - The object of blackjack is to obtain cards whose sum is as great as \n",
    "      possible without exceeding 21\n",
    "    - All face cards count as 10, and an ace can count either as 1 or as 11\n",
    "    - Each player competes independently against the dealer\n",
    "    - The game begins with two cards dealt to both dealer and player\n",
    "    - One of the dealer's cards is face up (showing card) and one is face down\n",
    "    \n",
    "    State Space (200 states total):\n",
    "    - Player's current sum (12-21): 10 possible values\n",
    "    - Dealer's showing card (Ace-10): 10 possible values  \n",
    "    - Whether player holds a usable ace (True/False): 2 possible values\n",
    "    - Total: 10 × 10 × 2 = 200 states\n",
    "    \n",
    "    Note: Player sums below 12 are not included because the player should \n",
    "    always hit with sum < 12 (no decision to be made)\n",
    "    \n",
    "    Reward Structure:\n",
    "    - +1 for winning\n",
    "    - -1 for losing\n",
    "    - 0 for draw\n",
    "    - All intermediate rewards are 0\n",
    "    - Discount factor γ = 1 (no discounting) as stated in textbook\n",
    "    \n",
    "    Environment Assumptions (from textbook):\n",
    "    - Cards are dealt from an infinite deck (with replacement)\n",
    "    - No advantage to keeping track of cards already dealt\n",
    "    \"\"\"\n",
    "    \n",
    "    # Action constants for readability\n",
    "    ACTION_STICK = 0\n",
    "    ACTION_HIT = 1\n",
    "    \n",
    "    # Dealer threshold - dealer must stick at this sum or higher\n",
    "    DEALER_STICK_THRESHOLD = 17\n",
    "    \n",
    "    # Blackjack winning total\n",
    "    BLACKJACK_TARGET = 21\n",
    "    \n",
    "    # Reward constants\n",
    "    REWARD_WIN = 1\n",
    "    REWARD_LOSS = -1\n",
    "    REWARD_DRAW = 0\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the blackjack environment\n",
    "        \n",
    "        For infinite deck assumption, we set up uniform probabilities\n",
    "        for cards 1-10, where:\n",
    "        - Card 1 represents Ace\n",
    "        - Cards 2-9 represent their face value\n",
    "        - Card 10 represents 10, J, Q, K (all valued at 10)\n",
    "        \"\"\"\n",
    "        # Current game state\n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "        \n",
    "        # Random number generator for reproducibility if needed\n",
    "        self.rng = np.random.default_rng()\n",
    "    \n",
    "    def draw_card(self):\n",
    "        \"\"\"\n",
    "        Draw a card from infinite deck\n",
    "        \n",
    "        Returns:\n",
    "            int: Card value (1-10, where 1 is Ace)\n",
    "        \n",
    "        Note from textbook: \n",
    "        - Cards are dealt with replacement (infinite deck assumption)\n",
    "        - This simplifies the problem by making draws independent\n",
    "        \n",
    "        Implementation details:\n",
    "        - We use discrete uniform distribution over [1, 10]\n",
    "        - Card value 10 represents any 10-value card (10, J, Q, K)\n",
    "        - This matches the textbook's simplified model\n",
    "        \"\"\"\n",
    "        # Draw uniformly from 1 to 10 (inclusive)\n",
    "        # In a real deck, 10s are more common, but textbook uses uniform\n",
    "        card_value = self.rng.integers(low=1, high=11)\n",
    "        return card_value\n",
    "    \n",
    "    def usable_ace(self, hand):\n",
    "        \"\"\"\n",
    "        Determine if the hand has a usable ace\n",
    "        \n",
    "        From textbook Section 5.1:\n",
    "        \"If the player holds an ace that he could count as 11 without going bust,\n",
    "        then the ace is said to be usable. In this case it is always counted as 11\n",
    "        because counting it as 1 would make the sum 11 or less, in which case there\n",
    "        is no decision to be made because, obviously, the player should always hit.\"\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if hand has usable ace, False otherwise\n",
    "            \n",
    "        Logic:\n",
    "        - An ace is \"usable\" if it can be counted as 11 without busting\n",
    "        - This means: (sum of all cards) + 10 <= 21\n",
    "        - The +10 comes from counting one ace as 11 instead of 1\n",
    "        \"\"\"\n",
    "        # Check if hand contains at least one ace (value 1)\n",
    "        has_ace = 1 in hand\n",
    "        \n",
    "        if not has_ace:\n",
    "            return False\n",
    "        \n",
    "        # Calculate sum with all aces counted as 1\n",
    "        current_sum = sum(hand)\n",
    "        \n",
    "        # Check if we can count one ace as 11 (adding 10 to current sum)\n",
    "        # without exceeding 21\n",
    "        can_use_ace_as_eleven = (current_sum + 10) <= self.BLACKJACK_TARGET\n",
    "        \n",
    "        return can_use_ace_as_eleven\n",
    "    \n",
    "    def sum_hand(self, hand):\n",
    "        \"\"\"\n",
    "        Calculate the sum of a hand, accounting for usable ace\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            int: Sum of hand (with ace counted optimally)\n",
    "            \n",
    "        Implementation note:\n",
    "        - Start by counting all aces as 1\n",
    "        - If there's a usable ace, add 10 to count it as 11\n",
    "        - This gives us the optimal sum without busting\n",
    "        \"\"\"\n",
    "        # Sum all cards (aces counted as 1)\n",
    "        total_sum = sum(hand)\n",
    "        \n",
    "        # If we have a usable ace, count it as 11 instead of 1\n",
    "        # This adds 10 to the total\n",
    "        if self.usable_ace(hand):\n",
    "            total_sum += 10\n",
    "        \n",
    "        return total_sum\n",
    "    \n",
    "    def is_bust(self, hand):\n",
    "        \"\"\"\n",
    "        Check if hand is bust (sum > 21)\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if bust, False otherwise\n",
    "        \"\"\"\n",
    "        return self.sum_hand(hand) > self.BLACKJACK_TARGET\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Start a new episode\n",
    "        \n",
    "        From textbook:\n",
    "        \"The game begins with two cards dealt to both dealer and player.\n",
    "        One of the dealer's cards is face up and the other is face down.\"\n",
    "        \n",
    "        Returns:\n",
    "            state: Tuple of (player_sum, dealer_showing, usable_ace)\n",
    "            \n",
    "        Implementation notes:\n",
    "        - Deal initial cards to both player and dealer\n",
    "        - If player gets natural (21), handle it according to rules\n",
    "        - Return the initial observable state\n",
    "        \"\"\"\n",
    "        # Deal two cards to player\n",
    "        self.player_hand = [self.draw_card(), self.draw_card()]\n",
    "        \n",
    "        # Deal two cards to dealer\n",
    "        # First card is \"showing\" (face up), second is \"hole card\" (face down)\n",
    "        self.dealer_hand = [self.draw_card(), self.draw_card()]\n",
    "        \n",
    "        # Extract dealer's showing card (first card by convention)\n",
    "        dealer_showing_card = self.dealer_hand[0]\n",
    "        \n",
    "        # Calculate player's sum and check for usable ace\n",
    "        player_total = self.sum_hand(self.player_hand)\n",
    "        has_usable_ace = self.usable_ace(self.player_hand)\n",
    "        \n",
    "        # Create state tuple: (player sum, dealer showing, usable ace)\n",
    "        initial_state = (player_total, dealer_showing_card, has_usable_ace)\n",
    "        \n",
    "        return initial_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment\n",
    "        \n",
    "        Args:\n",
    "            action: 0 = stick, 1 = hit\n",
    "            \n",
    "        Returns:\n",
    "            next_state: Next state after action (or None if terminal)\n",
    "            reward: Reward received\n",
    "            done: Whether episode is complete\n",
    "            \n",
    "        From textbook Section 5.1:\n",
    "        \"The player's actions are to hit or to stick.\"\n",
    "        \n",
    "        Implementation structure:\n",
    "        - If player hits: draw card, check for bust, return new state\n",
    "        - If player sticks: dealer plays, determine winner, return reward\n",
    "        \"\"\"\n",
    "        # CASE 1: Player chooses to HIT\n",
    "        if action == self.ACTION_HIT:\n",
    "            # Draw a new card and add to player's hand\n",
    "            new_card = self.draw_card()\n",
    "            self.player_hand.append(new_card)\n",
    "            \n",
    "            # Check if player went bust\n",
    "            if self.is_bust(self.player_hand):\n",
    "                # Player loses immediately\n",
    "                # Episode ends, no next state needed\n",
    "                return None, self.REWARD_LOSS, True\n",
    "            \n",
    "            # Player didn't bust - game continues\n",
    "            # Calculate new state\n",
    "            player_total = self.sum_hand(self.player_hand)\n",
    "            dealer_showing_card = self.dealer_hand[0]\n",
    "            has_usable_ace = self.usable_ace(self.player_hand)\n",
    "            \n",
    "            next_state = (player_total, dealer_showing_card, has_usable_ace)\n",
    "            \n",
    "            # No reward yet (intermediate reward is 0)\n",
    "            return next_state, 0, False\n",
    "        \n",
    "        # CASE 2: Player chooses to STICK\n",
    "        elif action == self.ACTION_STICK:\n",
    "            # Player's turn is over - now dealer plays\n",
    "            dealer_final_sum, dealer_busted = self.dealer_policy()\n",
    "            \n",
    "            # Calculate player's final sum\n",
    "            player_final_sum = self.sum_hand(self.player_hand)\n",
    "            \n",
    "            # Determine outcome and assign reward\n",
    "            if dealer_busted:\n",
    "                # Dealer went bust - player wins!\n",
    "                reward = self.REWARD_WIN\n",
    "            elif player_final_sum > dealer_final_sum:\n",
    "                # Player has higher sum - player wins!\n",
    "                reward = self.REWARD_WIN\n",
    "            elif player_final_sum < dealer_final_sum:\n",
    "                # Dealer has higher sum - player loses\n",
    "                reward = self.REWARD_LOSS\n",
    "            else:\n",
    "                # Same sum - it's a draw\n",
    "                reward = self.REWARD_DRAW\n",
    "            \n",
    "            # Episode is complete\n",
    "            return None, reward, True\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}. Must be 0 (stick) or 1 (hit)\")\n",
    "    \n",
    "    def dealer_policy(self):\n",
    "        \"\"\"\n",
    "        Dealer's fixed strategy\n",
    "        \n",
    "        From textbook Section 5.1:\n",
    "        \"The dealer hits or sticks according to a fixed strategy without choice:\n",
    "        he sticks on any sum of 17 or greater, and hits otherwise.\"\n",
    "        \n",
    "        Returns:\n",
    "            final_sum: Dealer's final hand sum\n",
    "            is_bust: Whether dealer went bust\n",
    "            \n",
    "        Implementation:\n",
    "        - Dealer keeps hitting until reaching threshold or busting\n",
    "        - No choice involved - purely deterministic based on current sum\n",
    "        \"\"\"\n",
    "        # Dealer plays out their hand\n",
    "        while True:\n",
    "            current_dealer_sum = self.sum_hand(self.dealer_hand)\n",
    "            \n",
    "            # Check if dealer should stick\n",
    "            if current_dealer_sum >= self.DEALER_STICK_THRESHOLD:\n",
    "                # Dealer sticks - return final sum and bust status\n",
    "                dealer_busted = current_dealer_sum > self.BLACKJACK_TARGET\n",
    "                return current_dealer_sum, dealer_busted\n",
    "            \n",
    "            # Dealer must hit (sum < 17)\n",
    "            new_card = self.draw_card()\n",
    "            self.dealer_hand.append(new_card)\n",
    "            \n",
    "            # Check if dealer busted after drawing\n",
    "            if self.is_bust(self.dealer_hand):\n",
    "                # Dealer went bust\n",
    "                final_sum = self.sum_hand(self.dealer_hand)\n",
    "                return final_sum, True\n",
    "    \n",
    "    def natural(self, hand):\n",
    "        \"\"\"\n",
    "        Check if hand is a natural (Ace + 10-card = 21 with 2 cards)\n",
    "        \n",
    "        From textbook:\n",
    "        \"If the player has 21 immediately (an ace and a 10-card), it is called \n",
    "        a natural. He then wins unless the dealer also has a natural, in which \n",
    "        case the game is a draw.\"\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if natural, False otherwise\n",
    "            \n",
    "        Logic:\n",
    "        - Must have exactly 2 cards\n",
    "        - Must sum to exactly 21\n",
    "        - Must contain an ace (value 1) and a 10-value card\n",
    "        \"\"\"\n",
    "        # Check for exactly 2 cards\n",
    "        if len(hand) != 2:\n",
    "            return False\n",
    "        \n",
    "        # Check if sum equals 21\n",
    "        # (use sum_hand to properly handle ace)\n",
    "        if self.sum_hand(hand) != self.BLACKJACK_TARGET:\n",
    "            return False\n",
    "        \n",
    "        # Check for ace and 10-card combination\n",
    "        # A natural requires one ace (1) and one 10-value card (10)\n",
    "        has_ace = 1 in hand\n",
    "        has_ten = 10 in hand\n",
    "        \n",
    "        return has_ace and has_ten\n",
    "\n",
    "\n",
    "def simple_policy(state):\n",
    "    \"\"\"\n",
    "    The policy to evaluate (from Figure 5.1 caption)\n",
    "    \n",
    "    From textbook Section 5.1:\n",
    "    \"Consider the policy that sticks if the player's sum is 20 or 21, \n",
    "    and otherwise hits.\"\n",
    "    \n",
    "    This is a fixed, deterministic policy:\n",
    "    - Stick (action 0) if player sum is 20 or 21\n",
    "    - Hit (action 1) otherwise\n",
    "    \n",
    "    Args:\n",
    "        state: Tuple of (player_sum, dealer_showing, usable_ace)\n",
    "        \n",
    "    Returns:\n",
    "        action: 0 = stick, 1 = hit\n",
    "    \"\"\"\n",
    "    # Unpack state tuple\n",
    "    player_sum, dealer_showing, usable_ace = state\n",
    "    \n",
    "    # Policy decision based on player's sum\n",
    "    if player_sum >= 20:\n",
    "        # Stick on 20 or 21\n",
    "        return BlackjackEnvironment.ACTION_STICK\n",
    "    else:\n",
    "        # Hit on anything less than 20\n",
    "        return BlackjackEnvironment.ACTION_HIT\n",
    "\n",
    "\n",
    "def first_visit_mc_prediction(policy, env, num_episodes, gamma=1.0):\n",
    "    \"\"\"\n",
    "    First-visit Monte Carlo policy evaluation\n",
    "    \n",
    "    Algorithm from Sutton & Barto Section 5.1 (page 92):\n",
    "    \n",
    "    Input: a policy π to be evaluated\n",
    "    Initialize:\n",
    "        V(s) ∈ ℝ, arbitrarily, for all s ∈ S\n",
    "        Returns(s) ← an empty list, for all s ∈ S\n",
    "    Loop forever (for each episode):\n",
    "        Generate an episode following π: S₀, A₀, R₁, S₁, A₁, R₂, ..., S_{T-1}, A_{T-1}, R_T\n",
    "        G ← 0\n",
    "        Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
    "            G ← γG + R_{t+1}\n",
    "            Unless S_t appears in S₀, S₁, ..., S_{t-1}:\n",
    "                Append G to Returns(S_t)\n",
    "                V(S_t) ← average(Returns(S_t))\n",
    "    \n",
    "    Key concepts from textbook:\n",
    "    - \"First-visit MC method estimates v_π(s) as the average of the returns \n",
    "       following first visits to s\"\n",
    "    - \"Each return is an independent, identically distributed estimate of v_π(s)\"\n",
    "    - \"By the law of large numbers the sequence of averages converges to the \n",
    "       expected value\"\n",
    "    - \"The standard deviation of error falls as 1/√n where n is the number of \n",
    "       returns averaged\"\n",
    "    \n",
    "    Args:\n",
    "        policy: Function mapping states to actions\n",
    "        env: BlackjackEnvironment instance\n",
    "        num_episodes: Number of episodes to run (10,000 or 500,000 for Figure 5.1)\n",
    "        gamma: Discount factor (1.0 for blackjack as per textbook)\n",
    "        \n",
    "    Returns:\n",
    "        V: Dictionary mapping states to estimated values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize value function arbitrarily (we use 0.0 for all states)\n",
    "    # V maps state -> estimated value\n",
    "    # Using defaultdict so unvisited states automatically have value 0.0\n",
    "    V = defaultdict(float)\n",
    "    \n",
    "    # Returns(s) stores list of returns following first visits to state s\n",
    "    # From textbook: We need to track all returns to compute the average\n",
    "    # This is the key data structure for Monte Carlo learning\n",
    "    # Each state maps to a list of all returns observed after first visits\n",
    "    Returns = defaultdict(list)\n",
    "    \n",
    "    # Track progress for long runs\n",
    "    if num_episodes >= 10000:\n",
    "        print(f\"Running First-Visit MC Prediction for {num_episodes} episodes...\")\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode_num in range(num_episodes):\n",
    "        \n",
    "        # GENERATE AN EPISODE following policy π\n",
    "        # Episode is sequence: S₀, A₀, R₁, S₁, A₁, R₂, ..., S_{T-1}, A_{T-1}, R_T\n",
    "        # We store tuples of (state, action, reward) for each time step\n",
    "        episode_history = []\n",
    "        \n",
    "        # Reset environment to get initial state S₀\n",
    "        current_state = env.reset()\n",
    "        \n",
    "        # Generate complete episode until terminal state\n",
    "        episode_complete = False\n",
    "        \n",
    "        while not episode_complete:\n",
    "            # Get action from policy for current state\n",
    "            action = policy(current_state)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, episode_complete = env.step(action)\n",
    "            \n",
    "            # Store this transition\n",
    "            # Note: We store (state, action, reward) where reward is R_{t+1}\n",
    "            # This matches the textbook notation: taking action A_t in state S_t\n",
    "            # gives us reward R_{t+1}\n",
    "            episode_history.append((current_state, action, reward))\n",
    "            \n",
    "            # Move to next state (if episode continues)\n",
    "            if not episode_complete:\n",
    "                current_state = next_state\n",
    "        \n",
    "        # CALCULATE RETURNS AND UPDATE VALUE FUNCTION\n",
    "        # Work backwards from end of episode\n",
    "        # This is the key insight: we know the final outcome, so we work backwards\n",
    "        # assigning returns to each state visited\n",
    "        \n",
    "        # G is the return (cumulative discounted reward from time t onward)\n",
    "        # Initialize to 0 and build up as we go backwards\n",
    "        cumulative_return = 0.0\n",
    "        \n",
    "        # Track which states we've already seen in this backward pass\n",
    "        # This is crucial for implementing \"first-visit\" MC\n",
    "        # We only want to update a state the FIRST time we see it (going backwards)\n",
    "        states_seen_in_reverse = set()\n",
    "        \n",
    "        # Loop backwards through episode (t = T-1, T-2, ..., 0)\n",
    "        # Going backwards means we process the END of the episode first\n",
    "        for step_index in range(len(episode_history) - 1, -1, -1):\n",
    "            \n",
    "            # Extract state, action, and reward from this time step\n",
    "            # episode_history[step_index] = (S_t, A_t, R_{t+1})\n",
    "            state_at_step, action_at_step, reward_received = episode_history[step_index]\n",
    "            \n",
    "            # Calculate return: G ← γ * G + R_{t+1}\n",
    "            # From textbook: For blackjack, γ = 1, so G ← G + R_{t+1}\n",
    "            # This accumulates rewards going backwards through the episode\n",
    "            cumulative_return = gamma * cumulative_return + reward_received\n",
    "            \n",
    "            # FIRST-VISIT CHECK\n",
    "            # \"Unless S_t appears in S₀, S₁, ..., S_{t-1}\"\n",
    "            # This is the key difference between first-visit and every-visit MC\n",
    "            # \n",
    "            # Going backwards: we only process a state if we haven't seen it yet\n",
    "            # in our backward traversal. This ensures we only count the FIRST\n",
    "            # visit to each state (chronologically first = last seen going backwards)\n",
    "            if state_at_step not in states_seen_in_reverse:\n",
    "                \n",
    "                # Mark this state as seen\n",
    "                states_seen_in_reverse.add(state_at_step)\n",
    "                \n",
    "                # Append return to Returns(S_t)\n",
    "                # This builds up our sample of returns for this state\n",
    "                Returns[state_at_step].append(cumulative_return)\n",
    "                \n",
    "                # Update value estimate: V(S_t) ← average(Returns(S_t))\n",
    "                # From textbook: \"The value estimate is the sample mean of returns\"\n",
    "                # As we collect more samples, this average converges to true value\n",
    "                V[state_at_step] = np.mean(Returns[state_at_step])\n",
    "        \n",
    "        # Optional: Print progress for long runs\n",
    "        if num_episodes >= 10000 and (episode_num + 1) % 10000 == 0:\n",
    "            print(f\"  Completed {episode_num + 1}/{num_episodes} episodes\")\n",
    "    \n",
    "    if num_episodes >= 10000:\n",
    "        print(f\"First-Visit MC Prediction complete!\")\n",
    "        print(f\"Total unique states visited: {len(V)}\")\n",
    "    \n",
    "    return V\n",
    "\n",
    "\n",
    "def first_visit_mc_prediction_with_counts(policy, env, num_episodes, gamma=1.0):\n",
    "    \"\"\"\n",
    "    First-visit Monte Carlo prediction with visit counts tracked\n",
    "    \n",
    "    This is an alternative implementation that tracks visit counts separately.\n",
    "    This can be more memory efficient for large numbers of episodes since we\n",
    "    don't store every single return - we just update a running average.\n",
    "    \n",
    "    Mathematically equivalent to the version above, but uses incremental updates:\n",
    "    V(S) ← V(S) + (1/N(S)) * [G - V(S)]\n",
    "    \n",
    "    where N(S) is the number of times we've visited state S.\n",
    "    \n",
    "    Args:\n",
    "        policy: Function mapping states to actions\n",
    "        env: BlackjackEnvironment instance\n",
    "        num_episodes: Number of episodes to run\n",
    "        gamma: Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        V: Dictionary mapping states to estimated values\n",
    "        visit_counts: Dictionary mapping states to number of visits\n",
    "    \"\"\"\n",
    "    \n",
    "    # Value function: state -> estimated value\n",
    "    V = defaultdict(float)\n",
    "    \n",
    "    # Visit counts: state -> number of first visits\n",
    "    # This tracks how many times we've updated each state's value\n",
    "    visit_counts = defaultdict(int)\n",
    "    \n",
    "    if num_episodes >= 10000:\n",
    "        print(f\"Running First-Visit MC Prediction (incremental version) for {num_episodes} episodes...\")\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        \n",
    "        # GENERATE EPISODE\n",
    "        episode_history = []\n",
    "        current_state = env.reset()\n",
    "        episode_complete = False\n",
    "        \n",
    "        while not episode_complete:\n",
    "            action = policy(current_state)\n",
    "            next_state, reward, episode_complete = env.step(action)\n",
    "            episode_history.append((current_state, action, reward))\n",
    "            \n",
    "            if not episode_complete:\n",
    "                current_state = next_state\n",
    "        \n",
    "        # PROCESS EPISODE BACKWARDS\n",
    "        cumulative_return = 0.0\n",
    "        states_seen_in_reverse = set()\n",
    "        \n",
    "        for step_index in range(len(episode_history) - 1, -1, -1):\n",
    "            state_at_step, action_at_step, reward_received = episode_history[step_index]\n",
    "            \n",
    "            # Accumulate return\n",
    "            cumulative_return = gamma * cumulative_return + reward_received\n",
    "            \n",
    "            # First-visit check\n",
    "            if state_at_step not in states_seen_in_reverse:\n",
    "                states_seen_in_reverse.add(state_at_step)\n",
    "                \n",
    "                # Increment visit count\n",
    "                visit_counts[state_at_step] += 1\n",
    "                \n",
    "                # Incremental update formula\n",
    "                # This is mathematically equivalent to taking the mean of all returns\n",
    "                # but only requires storing one value instead of a list\n",
    "                # \n",
    "                # Formula: V_new = V_old + (1/N) * (G - V_old)\n",
    "                # This is a \"moving average\" update\n",
    "                n = visit_counts[state_at_step]\n",
    "                current_value = V[state_at_step]\n",
    "                V[state_at_step] = current_value + (1.0 / n) * (cumulative_return - current_value)\n",
    "        \n",
    "        # Progress tracking\n",
    "        if num_episodes >= 10000 and (episode_num + 1) % 10000 == 0:\n",
    "            print(f\"  Completed {episode_num + 1}/{num_episodes} episodes\")\n",
    "    \n",
    "    if num_episodes >= 10000:\n",
    "        print(f\"First-Visit MC Prediction complete!\")\n",
    "        print(f\"Total unique states visited: {len(V)}\")\n",
    "    \n",
    "    return V, visit_counts\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def state_value_to_arrays(V, usable_ace):\n",
    "    \"\"\"\n",
    "    Convert state-value dictionary to 2D arrays for plotting\n",
    "    \n",
    "    The state space for plotting is:\n",
    "    - X-axis: Dealer showing card (1-10, representing Ace through 10)\n",
    "    - Y-axis: Player sum (12-21)\n",
    "    - Z-axis: Value V(s)\n",
    "    - Separate plots for usable ace vs. no usable ace\n",
    "    \n",
    "    Args:\n",
    "        V: Dictionary mapping (player_sum, dealer_showing, usable_ace) -> value\n",
    "        usable_ace: Boolean, which ace condition to extract\n",
    "        \n",
    "    Returns:\n",
    "        values: 2D numpy array of shape (10, 10) for plotting\n",
    "                rows = player sum (12-21)\n",
    "                cols = dealer showing (1-10)\n",
    "    \"\"\"\n",
    "    # TODO: Initialize 10x10 array (player sums 12-21, dealer showing 1-10)\n",
    "    # TODO: Extract values from V for given usable_ace condition\n",
    "    # TODO: Return 2D array suitable for surface plot\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_value_function(V, num_episodes, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create 3D surface plots matching Figure 5.1 layout\n",
    "    \n",
    "    Figure 5.1 shows:\n",
    "    - Left column: After 10,000 episodes\n",
    "    - Right column: After 500,000 episodes\n",
    "    - Top row: Usable ace\n",
    "    - Bottom row: No usable ace\n",
    "    \n",
    "    Each subplot shows:\n",
    "    - X-axis: Dealer showing (A, 2, 3, ..., 10)\n",
    "    - Y-axis: Player sum (12, 13, ..., 21)\n",
    "    - Z-axis: State value (approximately -1 to +1)\n",
    "    \n",
    "    The plots are wireframe/surface plots showing the value function as a 3D surface\n",
    "    \n",
    "    Args:\n",
    "        V: Value function dictionary\n",
    "        num_episodes: Number of episodes run (for title)\n",
    "        title_suffix: Additional text for title\n",
    "    \"\"\"\n",
    "    # TODO: Create figure with 2 subplots (usable ace, no usable ace)\n",
    "    \n",
    "    # TODO: For usable_ace in [True, False]:\n",
    "    #   - Convert V to 2D array\n",
    "    #   - Create meshgrid for X (dealer) and Y (player sum)\n",
    "    #   - Create 3D surface plot\n",
    "    #   - Set axis labels: \"Dealer showing\", \"Player sum\", value\n",
    "    #   - Set title: \"Usable ace\" or \"No usable ace\"\n",
    "    #   - Set z-axis limits approximately [-1, +1]\n",
    "    \n",
    "    # TODO: Add overall title: f\"After {num_episodes} episodes\"\n",
    "    # TODO: Adjust layout and show plot\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to recreate Figure 5.1\n",
    "    \n",
    "    From textbook Section 5.1:\n",
    "    \"In this way, we obtained the estimates of the state-value function shown \n",
    "    in Figure 5.1. The estimates for states with a usable ace are less certain \n",
    "    and less regular because these states are less common. In any event, after \n",
    "    500,000 games the value function is very well approximated.\"\n",
    "    \n",
    "    The figure shows results after:\n",
    "    1. 10,000 episodes (left column)\n",
    "    2. 500,000 episodes (right column)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Recreating Figure 5.1: Blackjack Monte Carlo Policy Evaluation\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize environment\n",
    "    # TODO: env = BlackjackEnvironment()\n",
    "    \n",
    "    # Run First-Visit MC for 10,000 episodes\n",
    "    print(\"Running First-Visit Monte Carlo Prediction...\")\n",
    "    print(\"Episodes: 10,000\")\n",
    "    # TODO: V_10k = first_visit_mc_prediction(simple_policy, env, 10000)\n",
    "    print(\"Completed 10,000 episodes\")\n",
    "    \n",
    "    # Plot results for 10,000 episodes\n",
    "    # TODO: plot_value_function(V_10k, 10000)\n",
    "    \n",
    "    # Run First-Visit MC for 500,000 episodes  \n",
    "    print(\"\\nEpisodes: 500,000\")\n",
    "    # TODO: V_500k = first_visit_mc_prediction(simple_policy, env, 500000)\n",
    "    print(\"Completed 500,000 episodes\")\n",
    "    \n",
    "    # Plot results for 500,000 episodes\n",
    "    # TODO: plot_value_function(V_500k, 500000)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO: main()\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
