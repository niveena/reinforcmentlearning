{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e653564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Niveen Abdul-Mohsen (bvn9ad)\n",
    "# Reinforcement Learning (CS 4771) - Fgure 5.1\n",
    "# Monte Carlo On policy\n",
    "# i used numpy for numerical operations and matplotlib for plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import defaultdict\n",
    "\n",
    "class BlackjackEnvironment:\n",
    "    \"\"\"\n",
    "    Blackjack Environment Implementation\n",
    "    \n",
    "    From Sutton & Barto Section 5.1, Example 5.1:\n",
    "    - The object of blackjack is to obtain cards whose sum is as great as \n",
    "      possible without exceeding 21\n",
    "    - All face cards count as 10, and an ace can count either as 1 or as 11\n",
    "    - Each player competes independently against the dealer\n",
    "    - The game begins with two cards dealt to both dealer and player\n",
    "    - One of the dealer's cards is face up (showing card) and one is face down\n",
    "    \n",
    "    State Space (200 states total):\n",
    "    - Player's current sum (12-21): 10 possible values\n",
    "    - Dealer's showing card (Ace-10): 10 possible values  \n",
    "    - Whether player holds a usable ace (True/False): 2 possible values\n",
    "    - Total: 10 × 10 × 2 = 200 states\n",
    "    \n",
    "    Note: Player sums below 12 are not included because the player should \n",
    "    always hit with sum < 12 (no decision to be made)\n",
    "    \n",
    "    Reward Structure:\n",
    "    - +1 for winning\n",
    "    - -1 for losing\n",
    "    - 0 for draw\n",
    "    - All intermediate rewards are 0\n",
    "    - Discount factor γ = 1 (no discounting) as stated in textbook\n",
    "    \n",
    "    Environment Assumptions (from textbook):\n",
    "    - Cards are dealt from an infinite deck (with replacement)\n",
    "    - No advantage to keeping track of cards already dealt\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the blackjack environment\n",
    "        \"\"\"\n",
    "        # TODO: Initialize deck probabilities for infinite deck assumption\n",
    "        # Each card from 1-10 has equal probability (with 10 representing 10, J, Q, K)\n",
    "        pass\n",
    "    \n",
    "    def draw_card(self):\n",
    "        \"\"\"\n",
    "        Draw a card from infinite deck\n",
    "        \n",
    "        Returns:\n",
    "            int: Card value (1-10, where 1 is Ace)\n",
    "        \n",
    "        Note from textbook: \n",
    "        - Cards are dealt with replacement (infinite deck assumption)\n",
    "        - This simplifies the problem by making draws independent\n",
    "        \"\"\"\n",
    "        # TODO: Implement random card draw\n",
    "        # Face cards (J, Q, K) all count as 10\n",
    "        # Ace is represented as 1 (can be counted as 11 via usable_ace flag)\n",
    "        pass\n",
    "    \n",
    "    def usable_ace(self, hand):\n",
    "        \"\"\"\n",
    "        Determine if the hand has a usable ace\n",
    "        \n",
    "        From textbook Section 5.1:\n",
    "        \"If the player holds an ace that he could count as 11 without going bust,\n",
    "        then the ace is said to be usable. In this case it is always counted as 11\n",
    "        because counting it as 1 would make the sum 11 or less, in which case there\n",
    "        is no decision to be made because, obviously, the player should always hit.\"\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if hand has usable ace, False otherwise\n",
    "        \"\"\"\n",
    "        # TODO: Check if hand contains an ace (value 1)\n",
    "        # TODO: Check if counting ace as 11 keeps sum <= 21\n",
    "        pass\n",
    "    \n",
    "    def sum_hand(self, hand):\n",
    "        \"\"\"\n",
    "        Calculate the sum of a hand, accounting for usable ace\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            int: Sum of hand (with ace counted optimally)\n",
    "        \"\"\"\n",
    "        # TODO: Calculate sum\n",
    "        # TODO: If usable ace exists, add 10 to sum (counting ace as 11 instead of 1)\n",
    "        pass\n",
    "    \n",
    "    def is_bust(self, hand):\n",
    "        \"\"\"\n",
    "        Check if hand is bust (sum > 21)\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if bust, False otherwise\n",
    "        \"\"\"\n",
    "        # TODO: Return True if sum_hand(hand) > 21\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Start a new episode\n",
    "        \n",
    "        From textbook:\n",
    "        \"The game begins with two cards dealt to both dealer and player.\n",
    "        One of the dealer's cards is face up and the other is face down.\"\n",
    "        \n",
    "        Returns:\n",
    "            state: Tuple of (player_sum, dealer_showing, usable_ace)\n",
    "        \"\"\"\n",
    "        # TODO: Deal two cards to player\n",
    "        # TODO: Deal two cards to dealer (one face up, one face down)\n",
    "        # TODO: Check for natural (immediate 21)\n",
    "        # TODO: Return initial state as (player_sum, dealer_showing, usable_ace)\n",
    "        pass\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment\n",
    "        \n",
    "        Args:\n",
    "            action: 0 = stick, 1 = hit\n",
    "            \n",
    "        Returns:\n",
    "            next_state: Next state after action\n",
    "            reward: Reward received\n",
    "            done: Whether episode is complete\n",
    "            \n",
    "        From textbook Section 5.1:\n",
    "        \"The player's actions are to hit or to stick.\"\n",
    "        \"\"\"\n",
    "        # TODO: If action is hit (1):\n",
    "        #   - Draw a card\n",
    "        #   - Add to player hand\n",
    "        #   - Check if bust (done=True, reward=-1)\n",
    "        #   - Update state\n",
    "        \n",
    "        # TODO: If action is stick (0):\n",
    "        #   - Dealer plays according to fixed policy\n",
    "        #   - Dealer hits on sum < 17, sticks on sum >= 17\n",
    "        #   - Determine winner and assign reward\n",
    "        #   - Set done=True\n",
    "        pass\n",
    "    \n",
    "    def dealer_policy(self):\n",
    "        \"\"\"\n",
    "        Dealer's fixed strategy\n",
    "        \n",
    "        From textbook Section 5.1:\n",
    "        \"The dealer hits or sticks according to a fixed strategy without choice:\n",
    "        he sticks on any sum of 17 or greater, and hits otherwise.\"\n",
    "        \n",
    "        Returns:\n",
    "            final_sum: Dealer's final hand sum\n",
    "            is_bust: Whether dealer went bust\n",
    "        \"\"\"\n",
    "        # TODO: Dealer hits until sum >= 17\n",
    "        # TODO: Return final sum and bust status\n",
    "        pass\n",
    "    \n",
    "    def natural(self, hand):\n",
    "        \"\"\"\n",
    "        Check if hand is a natural (Ace + 10-card = 21 with 2 cards)\n",
    "        \n",
    "        From textbook:\n",
    "        \"If the player has 21 immediately (an ace and a 10-card), it is called \n",
    "        a natural. He then wins unless the dealer also has a natural, in which \n",
    "        case the game is a draw.\"\n",
    "        \n",
    "        Args:\n",
    "            hand: List of card values\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if natural, False otherwise\n",
    "        \"\"\"\n",
    "        # TODO: Check if hand has exactly 2 cards\n",
    "        # TODO: Check if hand contains an ace and a 10-value card\n",
    "        # TODO: Check if sum equals 21\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def simple_policy(state):\n",
    "    \"\"\"\n",
    "    The policy to evaluate (from Figure 5.1 caption)\n",
    "    \n",
    "    From textbook Section 5.1:\n",
    "    \"Consider the policy that sticks if the player's sum is 20 or 21, \n",
    "    and otherwise hits.\"\n",
    "    \n",
    "    This is a fixed, deterministic policy:\n",
    "    - Stick (action 0) if player sum is 20 or 21\n",
    "    - Hit (action 1) otherwise\n",
    "    \n",
    "    Args:\n",
    "        state: Tuple of (player_sum, dealer_showing, usable_ace)\n",
    "        \n",
    "    Returns:\n",
    "        action: 0 = stick, 1 = hit\n",
    "    \"\"\"\n",
    "    # TODO: Extract player_sum from state\n",
    "    # TODO: Return 0 (stick) if player_sum >= 20\n",
    "    # TODO: Return 1 (hit) otherwise\n",
    "    pass\n",
    "\n",
    "def first_visit_mc_prediction(policy, env, num_episodes, gamma=1.0):\n",
    "    \"\"\"\n",
    "    First-visit Monte Carlo policy evaluation\n",
    "    \n",
    "    Algorithm from Sutton & Barto Section 5.1 (page 92):\n",
    "    \n",
    "    Input: a policy π to be evaluated\n",
    "    Initialize:\n",
    "        V(s) ∈ ℝ, arbitrarily, for all s ∈ S\n",
    "        Returns(s) ← an empty list, for all s ∈ S\n",
    "    Loop forever (for each episode):\n",
    "        Generate an episode following π: S₀, A₀, R₁, S₁, A₁, R₂, ..., S_{T-1}, A_{T-1}, R_T\n",
    "        G ← 0\n",
    "        Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
    "            G ← γG + R_{t+1}\n",
    "            Unless S_t appears in S₀, S₁, ..., S_{t-1}:\n",
    "                Append G to Returns(S_t)\n",
    "                V(S_t) ← average(Returns(S_t))\n",
    "    \n",
    "    Key concepts from textbook:\n",
    "    - \"First-visit MC method estimates v_π(s) as the average of the returns \n",
    "       following first visits to s\"\n",
    "    - \"Each return is an independent, identically distributed estimate of v_π(s)\"\n",
    "    - \"By the law of large numbers the sequence of averages converges to the \n",
    "       expected value\"\n",
    "    - \"The standard deviation of error falls as 1/√n where n is the number of \n",
    "       returns averaged\"\n",
    "    \n",
    "    Args:\n",
    "        policy: Function mapping states to actions\n",
    "        env: BlackjackEnvironment instance\n",
    "        num_episodes: Number of episodes to run (10,000 or 500,000 for Figure 5.1)\n",
    "        gamma: Discount factor (1.0 for blackjack as per textbook)\n",
    "        \n",
    "    Returns:\n",
    "        V: Dictionary mapping states to estimated values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize value function arbitrarily\n",
    "    # V maps state -> estimated value\n",
    "    # TODO: Initialize V as defaultdict(float) or nested dict\n",
    "    V = None\n",
    "    \n",
    "    # Returns(s) stores list of returns following first visits to state s\n",
    "    # From textbook: We need to track all returns to compute the average\n",
    "    # TODO: Initialize Returns as defaultdict(list) or nested dict\n",
    "    Returns = None\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode_num in range(num_episodes):\n",
    "        \n",
    "        # GENERATE AN EPISODE following policy π\n",
    "        # Episode is sequence: S₀, A₀, R₁, S₁, A₁, R₂, ..., S_{T-1}, A_{T-1}, R_T\n",
    "        # TODO: Initialize episode list to store (state, action, reward) tuples\n",
    "        episode = []\n",
    "        \n",
    "        # TODO: Reset environment to get initial state S₀\n",
    "        state = None\n",
    "        \n",
    "        # TODO: Generate episode until terminal state\n",
    "        # Loop:\n",
    "        #   - Get action from policy\n",
    "        #   - Take step in environment\n",
    "        #   - Store (state, action, reward) in episode\n",
    "        #   - Update state\n",
    "        #   - Break if done\n",
    "        \n",
    "        # CALCULATE RETURNS AND UPDATE VALUE FUNCTION\n",
    "        # Work backwards from end of episode\n",
    "        # G is the return (cumulative discounted reward)\n",
    "        G = 0\n",
    "        \n",
    "        # TODO: Loop backwards through episode (t = T-1, T-2, ..., 0)\n",
    "        # for t in range(len(episode)-1, -1, -1):\n",
    "        \n",
    "            # TODO: Extract state and reward from episode[t]\n",
    "            \n",
    "            # Calculate return: G ← γG + R_{t+1}\n",
    "            # From textbook: For blackjack, γ = 1, so G ← G + R_{t+1}\n",
    "            # TODO: Update G\n",
    "            \n",
    "            # FIRST-VISIT CHECK\n",
    "            # \"Unless S_t appears in S₀, S₁, ..., S_{t-1}\"\n",
    "            # This is the key difference between first-visit and every-visit MC\n",
    "            # TODO: Check if current state appeared earlier in episode\n",
    "            # if state not in [episode[i] for i in range(t)]:\n",
    "            \n",
    "                # Append return to Returns(S_t)\n",
    "                # TODO: Append G to Returns[state]\n",
    "                \n",
    "                # Update value estimate: V(S_t) ← average(Returns(S_t))\n",
    "                # From textbook: \"The value estimate is the sample mean of returns\"\n",
    "                # TODO: V[state] = mean(Returns[state])\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def state_value_to_arrays(V, usable_ace):\n",
    "    \"\"\"\n",
    "    Convert state-value dictionary to 2D arrays for plotting\n",
    "    \n",
    "    The state space for plotting is:\n",
    "    - X-axis: Dealer showing card (1-10, representing Ace through 10)\n",
    "    - Y-axis: Player sum (12-21)\n",
    "    - Z-axis: Value V(s)\n",
    "    - Separate plots for usable ace vs. no usable ace\n",
    "    \n",
    "    Args:\n",
    "        V: Dictionary mapping (player_sum, dealer_showing, usable_ace) -> value\n",
    "        usable_ace: Boolean, which ace condition to extract\n",
    "        \n",
    "    Returns:\n",
    "        values: 2D numpy array of shape (10, 10) for plotting\n",
    "                rows = player sum (12-21)\n",
    "                cols = dealer showing (1-10)\n",
    "    \"\"\"\n",
    "    # TODO: Initialize 10x10 array (player sums 12-21, dealer showing 1-10)\n",
    "    # TODO: Extract values from V for given usable_ace condition\n",
    "    # TODO: Return 2D array suitable for surface plot\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_value_function(V, num_episodes, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create 3D surface plots matching Figure 5.1 layout\n",
    "    \n",
    "    Figure 5.1 shows:\n",
    "    - Left column: After 10,000 episodes\n",
    "    - Right column: After 500,000 episodes\n",
    "    - Top row: Usable ace\n",
    "    - Bottom row: No usable ace\n",
    "    \n",
    "    Each subplot shows:\n",
    "    - X-axis: Dealer showing (A, 2, 3, ..., 10)\n",
    "    - Y-axis: Player sum (12, 13, ..., 21)\n",
    "    - Z-axis: State value (approximately -1 to +1)\n",
    "    \n",
    "    The plots are wireframe/surface plots showing the value function as a 3D surface\n",
    "    \n",
    "    Args:\n",
    "        V: Value function dictionary\n",
    "        num_episodes: Number of episodes run (for title)\n",
    "        title_suffix: Additional text for title\n",
    "    \"\"\"\n",
    "    # TODO: Create figure with 2 subplots (usable ace, no usable ace)\n",
    "    \n",
    "    # TODO: For usable_ace in [True, False]:\n",
    "    #   - Convert V to 2D array\n",
    "    #   - Create meshgrid for X (dealer) and Y (player sum)\n",
    "    #   - Create 3D surface plot\n",
    "    #   - Set axis labels: \"Dealer showing\", \"Player sum\", value\n",
    "    #   - Set title: \"Usable ace\" or \"No usable ace\"\n",
    "    #   - Set z-axis limits approximately [-1, +1]\n",
    "    \n",
    "    # TODO: Add overall title: f\"After {num_episodes} episodes\"\n",
    "    # TODO: Adjust layout and show plot\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to recreate Figure 5.1\n",
    "    \n",
    "    From textbook Section 5.1:\n",
    "    \"In this way, we obtained the estimates of the state-value function shown \n",
    "    in Figure 5.1. The estimates for states with a usable ace are less certain \n",
    "    and less regular because these states are less common. In any event, after \n",
    "    500,000 games the value function is very well approximated.\"\n",
    "    \n",
    "    The figure shows results after:\n",
    "    1. 10,000 episodes (left column)\n",
    "    2. 500,000 episodes (right column)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Recreating Figure 5.1: Blackjack Monte Carlo Policy Evaluation\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize environment\n",
    "    # TODO: env = BlackjackEnvironment()\n",
    "    \n",
    "    # Run First-Visit MC for 10,000 episodes\n",
    "    print(\"Running First-Visit Monte Carlo Prediction...\")\n",
    "    print(\"Episodes: 10,000\")\n",
    "    # TODO: V_10k = first_visit_mc_prediction(simple_policy, env, 10000)\n",
    "    print(\"Completed 10,000 episodes\")\n",
    "    \n",
    "    # Plot results for 10,000 episodes\n",
    "    # TODO: plot_value_function(V_10k, 10000)\n",
    "    \n",
    "    # Run First-Visit MC for 500,000 episodes  \n",
    "    print(\"\\nEpisodes: 500,000\")\n",
    "    # TODO: V_500k = first_visit_mc_prediction(simple_policy, env, 500000)\n",
    "    print(\"Completed 500,000 episodes\")\n",
    "    \n",
    "    # Plot results for 500,000 episodes\n",
    "    # TODO: plot_value_function(V_500k, 500000)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO: main()\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
