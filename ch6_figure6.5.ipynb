{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32275b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niveen Abdul-Mohsen (bvn9ad)\n",
    "# Reinforcement Learning (CS 4771) - Figure 6.5 Maximization Bias Q-learning and Double Q-learning \n",
    "# Comparison of Q-learning and Double Q-learning on a simple episodic MDP environment\n",
    "# i used numpy for numerical operations and matplotlib for plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MaxBiasEnvironment:\n",
    "    \"\"\"\n",
    "    simple two-state episodic environment that demonstrates maximization bias.\n",
    "    \n",
    "    structure:\n",
    "    - start in state A with two actions: left and right\n",
    "    - left action: transition to state B with reward 0\n",
    "    - from B: many actions all terminate with reward ~ N(-0.1, 1)\n",
    "    - right action: immediate termination with reward 0\n",
    "    \n",
    "    the optimal policy is to take RIGHT (avoiding the noisy negative rewards).\n",
    "    but q-learning with maximization bias will overestimate the value of\n",
    "    going LEFT because of noise in the B rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=None):\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.current_state = 'A'\n",
    "        self.is_terminal = False\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"start a new episode in state A\"\"\"\n",
    "        self.current_state = 'A'\n",
    "        self.is_terminal = False\n",
    "        return 'A'\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        execute one step in the environment\n",
    "        \n",
    "        returns: (next_state, reward, is_terminal)\n",
    "        \"\"\"\n",
    "        if self.is_terminal:\n",
    "            raise RuntimeError(\"episode already terminated\")\n",
    "        \n",
    "        if self.current_state == 'A':\n",
    "            if action == 0:  # left action\n",
    "                # transition to state B with zero immediate reward\n",
    "                self.current_state = 'B'\n",
    "                return ('B', 0.0, False)\n",
    "            else:  # right action (action == 1)\n",
    "                # immediate termination with zero reward (optimal choice)\n",
    "                self.is_terminal = True\n",
    "                return (None, 0.0, True)\n",
    "        \n",
    "        elif self.current_state == 'B':\n",
    "            # any action in B terminates\n",
    "            # reward drawn from N(-0.1, 1.0) - typically negative!\n",
    "            reward = self.rng.normal(loc=-0.1, scale=1.0)\n",
    "            self.is_terminal = True\n",
    "            return (None, reward, True)\n",
    "    \n",
    "    def get_num_states(self):\n",
    "        \"\"\"return total number of non-terminal states\"\"\"\n",
    "        return 2\n",
    "    \n",
    "    def get_num_actions(self):\n",
    "        \"\"\"return number of actions available in state A\"\"\"\n",
    "        return 2\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ALGORITHM: q-learning (with maximization bias)\n",
    "# ============================================================================\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    standard q-learning with epsilon-greedy exploration.\n",
    "    \n",
    "    key equation: Q(s, a) += alpha * [r + gamma * max_a' Q(s', a') - Q(s, a)]\n",
    "    \n",
    "    problem: uses the SAME estimate for both:\n",
    "    1) selecting which action maximizes value in next state\n",
    "    2) estimating that maximum value\n",
    "    \n",
    "    in state B with many actions, the max of noisy estimates is biased upward\n",
    "    even though true max is negative. this causes Q(A, left) to be overestimated.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_states, num_actions, alpha=0.1, gamma=1.0, epsilon=0.1):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.rng = np.random.RandomState()\n",
    "        \n",
    "        # q-values indexed by state name ('A' or 'B')\n",
    "        # state A has 2 actions, state B has 10 possible actions\n",
    "        self.q_values = {\n",
    "            'A': np.zeros(num_actions),      # left (0) and right (1)\n",
    "            'B': np.zeros(10)                 # 10 actions in state B\n",
    "        }\n",
    "    \n",
    "    def set_seed(self, seed):\n",
    "        \"\"\"set random seed for reproducibility\"\"\"\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        epsilon-greedy action selection\n",
    "        \n",
    "        with probability epsilon: select random action\n",
    "        otherwise: select action with highest q-value\n",
    "        \"\"\"\n",
    "        if self.rng.rand() < self.epsilon:\n",
    "            # explore: random action\n",
    "            if state == 'A':\n",
    "                return self.rng.randint(0, self.num_actions)\n",
    "            else:  # state B\n",
    "                return self.rng.randint(0, 10)\n",
    "        else:\n",
    "            # exploit: greedy action (highest q-value)\n",
    "            if state == 'A':\n",
    "                # break ties randomly\n",
    "                q_vals = self.q_values['A']\n",
    "                max_q = np.max(q_vals)\n",
    "                best_actions = np.where(q_vals == max_q)[0]\n",
    "                return self.rng.choice(best_actions)\n",
    "            else:  # state B\n",
    "                q_vals = self.q_values['B']\n",
    "                max_q = np.max(q_vals)\n",
    "                best_actions = np.where(q_vals == max_q)[0]\n",
    "                return self.rng.choice(best_actions)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_action=None):\n",
    "        \"\"\"\n",
    "        q-learning update rule\n",
    "        \n",
    "        uses max of next_state values as bootstrap target\n",
    "        this is where maximization bias occurs: the max of noisy estimates\n",
    "        in state B tends to be positive even though true values are ~-0.1\n",
    "        \"\"\"\n",
    "        if next_state is None:\n",
    "            # terminal state: bootstrap value is just the reward\n",
    "            target = reward\n",
    "        else:\n",
    "            # non-terminal: bootstrap using max q-value in next state\n",
    "            if next_state == 'A':\n",
    "                max_next_q = np.max(self.q_values['A'])\n",
    "            else:  # next_state == 'B'\n",
    "                max_next_q = np.max(self.q_values['B'])\n",
    "            \n",
    "            target = reward + self.gamma * max_next_q\n",
    "        \n",
    "        # temporal difference error\n",
    "        td_error = target - self.q_values[state][action]\n",
    "        \n",
    "        # update q-value\n",
    "        self.q_values[state][action] += self.alpha * td_error\n",
    "    \n",
    "    def get_greedy_action_from_a(self):\n",
    "        \"\"\"\n",
    "        return greedy action from state A (0=left, 1=right)\n",
    "        based on current q-value estimates\n",
    "        \"\"\"\n",
    "        q_left = self.q_values['A'][0]\n",
    "        q_right = self.q_values['A'][1]\n",
    "        \n",
    "        if q_left > q_right:\n",
    "            return 0  # choose left\n",
    "        elif q_right > q_left:\n",
    "            return 1  # choose right\n",
    "        else:\n",
    "            # tied: random choice\n",
    "            return self.rng.choice([0, 1])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ALGORITHM: double q-learning (corrects maximization bias)\n",
    "# ============================================================================\n",
    "\n",
    "class DoubleQLearningAgent:\n",
    "    \"\"\"\n",
    "    double q-learning algorithm that addresses maximization bias.\n",
    "    \n",
    "    maintain TWO independent q-value estimates (Q1 and Q2).\n",
    "    on each step, randomly update one using the other.\n",
    "    \n",
    "    update for Q1: Q1(s,a) += alpha * [r + gamma * Q2(argmax_a' Q1(s',a')) - Q1(s,a)]\n",
    "    update for Q2: Q2(s,a) += alpha * [r + gamma * Q1(argmax_a' Q2(s',a')) - Q2(s,a)]\n",
    "    \n",
    "    by decoupling which estimate selects the action (Q1 or Q2) from which\n",
    "    one estimates its value, the bias cancels out.\n",
    "    \n",
    "    if Q1 gets lucky and overestimates, Q2 (learned\n",
    "    independently) will tend to underestimate. averaging them (or alternating)\n",
    "    cancels out the bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_states, num_actions, alpha=0.1, gamma=1.0, epsilon=0.1):\n",
    "        pass\n",
    "    \n",
    "    def set_seed(self, seed):\n",
    "        \"\"\"set random seed for reproducibility\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        epsilon-greedy using average of both q-estimates\n",
    "        \n",
    "        averaging both estimates for action selection provides more\n",
    "        stable behavior than using a single estimate\n",
    "        \"\"\"\n",
    "        # TODO: if random number < epsilon:\n",
    "        #   - if state == 'A': return random action from [0, 1]\n",
    "        #   - else: return random action from [0, 10)\n",
    "        \n",
    "        # TODO: else (exploit):\n",
    "        #   - compute average q-value: (self.q_values_1[state] + self.q_values_2[state]) / 2.0\n",
    "        #   - find max of average q-value\n",
    "        #   - find ALL actions that achieve this max\n",
    "        #   - randomly choose among them\n",
    "        pass\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_action=None):\n",
    "        \"\"\"\n",
    "        double q-learning update\n",
    "        \n",
    "        randomly choose to update either Q1 or Q2 on this step\n",
    "        \"\"\"\n",
    "        # TODO: randomly decide: should we update Q1 or Q2? (50/50 chance)\n",
    "        \n",
    "        # TODO: if updating Q1:\n",
    "        #   - if next_state is None: target = reward\n",
    "        #   - else:\n",
    "        #     - find best action in next_state using Q1: argmax(Q1[next_state])\n",
    "        #     - get value of that action from Q2: Q2[next_state][best_action]\n",
    "        #     - target = reward + gamma * bootstrap_value\n",
    "        #   - calculate and apply td_error to Q1[state][action]\n",
    "        \n",
    "        # TODO: else (updating Q2):\n",
    "        #   - if next_state is None: target = reward\n",
    "        #   - else:\n",
    "        #     - find best action in next_state using Q2: argmax(Q2[next_state])\n",
    "        #     - get value of that action from Q1: Q1[next_state][best_action]\n",
    "        #     - target = reward + gamma * bootstrap_value\n",
    "        #   - calculate and apply td_error to Q2[state][action]\n",
    "        pass\n",
    "    \n",
    "    def get_greedy_action_from_a(self):\n",
    "        \"\"\"\n",
    "        return greedy action from state A (0=left, 1=right)\n",
    "        based on average of both q-value estimates\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def run_episode(agent, env):\n",
    "    \"\"\"\n",
    "    run one complete episode with given agent and environment\n",
    "    \"\"\"\n",
    "    # TODO: reset environment, get initial state = 'A'\n",
    "    # TODO: initialize action_in_a = None\n",
    "    \n",
    "    # TODO: while not env.is_terminal:\n",
    "    #   - agent gets action using epsilon-greedy\n",
    "    #   - if state == 'A': store this action in action_in_a\n",
    "    #   - environment executes action -> get next_state, reward, is_terminal\n",
    "    #   - agent updates q-values with this experience\n",
    "    #   - move to next_state\n",
    "    \n",
    "    # TODO: return action_in_a (the action taken from state A)\n",
    "    pass\n",
    "\n",
    "\n",
    "def run_experiment(num_episodes, num_runs, algorithm_type='qlearning'):\n",
    "    \"\"\"\n",
    "    run complete experiment comparing algorithms\n",
    "    \n",
    "    tracks which action is taken from state A in each episode.\n",
    "    we want to track LEFT action percentage (action 0).\n",
    " \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: for each run:\n",
    "    #   - create fresh environment with seed=run_idx\n",
    "    #   - create fresh agent (either QLearningAgent or DoubleQLearningAgent)\n",
    "    #   - set agent seed\n",
    "    #   - for each episode:\n",
    "    #     - run_episode with this agent and environment\n",
    "    #     - store whether action was LEFT (action 0) or not\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def create_figure_6_5():\n",
    "    \"\"\"\n",
    "    recreate figure 6.5 from the textbook\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"figure 6.5: maximization bias - q-learning vs double q-learning\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # experimental parameters\n",
    "    num_episodes = 300\n",
    "    num_runs = 10000\n",
    "    \n",
    "    print(f\"\\nrunning experiment with:\")\n",
    "    print(f\"  episodes per run: {num_episodes}\")\n",
    "    print(f\"  number of independent runs: {num_runs}\")\n",
    "    \n",
    "    # TODO: run experiments for both algorithms\n",
    "    \n",
    "    # TODO: create figure and plot\n",
    "    \n",
    "    # TODO: formatting\n",
    "\n",
    "    \n",
    "    # TODO: save figure and show\n",
    "    pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO: call create_figure_6_5() to run the full experiment\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
