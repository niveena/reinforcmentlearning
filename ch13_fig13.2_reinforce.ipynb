{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8bcbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niveen Abdul-Mohsen (bvn9ad)\n",
    "# Reinforcement Learning (CS 4771) - Figure 13.2 REINFORCE vs REINFORCE with Baseline\n",
    "# This is the code for the right graph of Figure 13.2 in Sutton and Barto's RL textbook\n",
    "# i used numpy for numerical operations and matplotlib for plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleCorridorEnv:\n",
    "    \"\"\"\n",
    "    Simple corridor gridworld with state 1 having reversed action effects.\n",
    "    \n",
    "    States: 0 (start), 1, 2, 3 (terminal)\n",
    "    Normal state mechanics: right increases state, left decreases\n",
    "    State 1 exception: right decreases, left increases (asymmetry!)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to starting state\"\"\"\n",
    "        self.state = 0\n",
    "    \n",
    "    def take_action(self, action_is_right):\n",
    "        \"\"\"\n",
    "        Execute one action step.\n",
    "        \n",
    "        Args:\n",
    "            action_is_right (bool): True for right action, False for left\n",
    "        \n",
    "        Returns:\n",
    "            reward (int): -1 for each step, 0 when done\n",
    "            is_done (bool): True if reached terminal state (state 3)\n",
    "        \"\"\"\n",
    "        \n",
    "        # States 0 and 2 have normal action effects\n",
    "        if self.state == 0 or self.state == 2:\n",
    "            if action_is_right:\n",
    "                self.state += 1\n",
    "            else:\n",
    "                self.state = max(0, self.state - 1)\n",
    "        \n",
    "        # State 1 has reversed action effects (the asymmetry)\n",
    "        elif self.state == 1:\n",
    "            if action_is_right:\n",
    "                self.state -= 1  # right actually goes left!\n",
    "            else:\n",
    "                self.state += 1  # left actually goes right!\n",
    "        \n",
    "        # Check if terminal\n",
    "        if self.state == 3:\n",
    "            return 0, True  # terminal reward is 0\n",
    "        else:\n",
    "            return -1, False  # step penalty\n",
    "        \n",
    "class PolicyGradientAgent:\n",
    "    \"\"\"\n",
    "    REINFORCE agent with optional learned baseline.\n",
    "    \n",
    "    Features softmax policy parameterized with linear preference function:\n",
    "        h = theta^T * x(state)\n",
    "        pi(action|state) = softmax(h)\n",
    "    \n",
    "    where x represents actions as one-hot features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, step_size_policy, discount=1.0, step_size_baseline=None):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            step_size_policy (float): Alpha for policy parameter updates\n",
    "            discount (float): Gamma (discount factor)\n",
    "            step_size_baseline (float): Alpha_w for baseline updates (optional)\n",
    "        \"\"\"\n",
    "        # Policy parameters theta (one for each action)\n",
    "        self.theta = np.array([-1.47, 1.47])\n",
    "        \n",
    "        # Feature matrix: rows=states, cols=actions\n",
    "        # Each state has one-hot action features\n",
    "        self.features = np.array([\n",
    "            [0, 1],  # state 0: [left_feature, right_feature]\n",
    "            [1, 0]   # state 1: [left_feature, right_feature]\n",
    "        ])\n",
    "        \n",
    "        self.step_size_policy = step_size_policy\n",
    "        self.discount = discount\n",
    "        self.step_size_baseline = step_size_baseline if step_size_baseline else 0\n",
    "        self.has_baseline = step_size_baseline is not None\n",
    "        \n",
    "        # Learned state value baseline\n",
    "        self.baseline_value = 0.0\n",
    "        \n",
    "        # Store trajectory for learning\n",
    "        self.episode_rewards = []\n",
    "        self.episode_actions = []\n",
    "    \n",
    "    def compute_policy(self):\n",
    "        \"\"\"\n",
    "        Compute action probabilities using softmax.\n",
    "        \n",
    "        Returns:\n",
    "            pmf (array): [prob_left, prob_right]\n",
    "        \"\"\"\n",
    "        # Compute preferences: h = theta^T * features\n",
    "        h = np.dot(self.theta, self.features)\n",
    "        \n",
    "        # Softmax with numerical stability\n",
    "        h_stable = h - np.max(h)\n",
    "        exp_h = np.exp(h_stable)\n",
    "        pmf = exp_h / np.sum(exp_h)\n",
    "        \n",
    "        # Ensure exploration (never fully deterministic)\n",
    "        epsilon = 0.05\n",
    "        min_idx = np.argmin(pmf)\n",
    "        if pmf[min_idx] < epsilon:\n",
    "            pmf[:] = 1.0 - epsilon\n",
    "            pmf[min_idx] = epsilon\n",
    "        \n",
    "        return pmf\n",
    "    \n",
    "    def select_action(self, step_reward):\n",
    "        \"\"\"\n",
    "        Select action according to policy and record step.\n",
    "        \n",
    "        Args:\n",
    "            step_reward (float or None): Reward from previous step\n",
    "        \n",
    "        Returns:\n",
    "            action_is_right (bool): Action choice\n",
    "        \"\"\"\n",
    "        if step_reward is not None:\n",
    "            self.episode_rewards.append(step_reward)\n",
    "        \n",
    "        pmf = self.compute_policy()\n",
    "        action_is_right = np.random.uniform() < pmf[1]\n",
    "        self.episode_actions.append(action_is_right)\n",
    "        \n",
    "        return action_is_right\n",
    "    \n",
    "    def learn_from_episode(self, final_reward):\n",
    "        \"\"\"\n",
    "        Update parameters after episode completes.\n",
    "        \n",
    "        Args:\n",
    "            final_reward (float): Final step reward\n",
    "        \"\"\"\n",
    "        self.episode_rewards.append(final_reward)\n",
    "        \n",
    "        # Compute returns G_t (backward from episode end)\n",
    "        num_steps = len(self.episode_rewards)\n",
    "        returns = np.zeros(num_steps)\n",
    "        returns[-1] = self.episode_rewards[-1]\n",
    "        \n",
    "        for step_idx in range(num_steps - 2, -1, -1):\n",
    "            returns[step_idx] = self.episode_rewards[step_idx] + self.discount * returns[step_idx + 1]\n",
    "        \n",
    "        # Update parameters for each step\n",
    "        discount_power = 1.0\n",
    "        \n",
    "        for step_idx in range(num_steps):\n",
    "            action_idx = 1 if self.episode_actions[step_idx] else 0\n",
    "            return_t = returns[step_idx]\n",
    "            \n",
    "            pmf = self.compute_policy()\n",
    "            \n",
    "            # Score function gradient: nabla log pi(a|s) = x(a) - E[x]\n",
    "            score_grad = self.features[:, action_idx] - np.dot(self.features, pmf)\n",
    "            \n",
    "            if self.has_baseline:\n",
    "                # Advantage: return minus baseline\n",
    "                advantage = return_t - self.baseline_value\n",
    "                \n",
    "                # Update baseline: w <- w + alpha_w * gamma^t * (G_t - w)\n",
    "                self.baseline_value += self.step_size_baseline * discount_power * advantage\n",
    "                \n",
    "                # Update policy: theta <- theta + alpha * gamma^t * advantage * nabla_log_pi\n",
    "                self.theta += self.step_size_policy * discount_power * advantage * score_grad\n",
    "            else:\n",
    "                # No baseline: update only on return\n",
    "                self.theta += self.step_size_policy * discount_power * return_t * score_grad\n",
    "            \n",
    "            discount_power *= self.discount\n",
    "        \n",
    "        # Clear trajectory\n",
    "        self.episode_rewards = []\n",
    "        self.episode_actions = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
