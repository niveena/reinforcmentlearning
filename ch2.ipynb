{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0f1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-armed bandit parameter study - function-based implementation\n",
      "recreating sutton & barto figure 2.6 using modular functions\n",
      "\n",
      "testing epsilon-greedy...\n",
      "  parameter 2^-7.0 = 0.007812\n",
      "  parameter 2^-6.0 = 0.015625\n"
     ]
    }
   ],
   "source": [
    "# Niveen Abdul-Mohsen (bvn9ad)\n",
    "# Reinforcement Learning (CS 4771) - Multi-armed Bandit Problem\n",
    "# multi-armed bandit: figure 2.6 parameter study - function-based implementation\n",
    "# i used numpy for numerical operations and matplotlib for plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_bandit_environment(arm_count=10, reward_center=0.0):\n",
    "    \"\"\"\n",
    "    creates a new bandit problem with random true arm values\n",
    "    returns the true reward values for each arm\n",
    "    \"\"\"\n",
    "    true_reward_values = np.random.randn(arm_count) + reward_center\n",
    "    return true_reward_values\n",
    "\n",
    "def get_noisy_reward(arm_index, true_rewards):\n",
    "    \"\"\"\n",
    "    generates a noisy reward for pulling a specific arm\n",
    "    reward is normally distributed around the true arm value\n",
    "    \"\"\"\n",
    "    return np.random.randn() + true_rewards[arm_index]\n",
    "\n",
    "def select_epsilon_greedy_arm(value_estimates, exploration_rate):\n",
    "    \"\"\"\n",
    "    chooses arm using epsilon-greedy strategy\n",
    "    explores with probability exploration_rate, otherwise picks best estimated arm\n",
    "    \"\"\"\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        return np.random.choice(len(value_estimates))\n",
    "    else:\n",
    "        max_value = np.max(value_estimates)\n",
    "        best_arms = np.where(value_estimates == max_value)[0]\n",
    "        return np.random.choice(best_arms)\n",
    "\n",
    "def select_ucb_arm(value_estimates, pull_counts, timestep, confidence_level):\n",
    "    \"\"\"\n",
    "    chooses arm using upper confidence bound\n",
    "    adds confidence bonus based on uncertainty and exploration need\n",
    "    \"\"\"\n",
    "    if timestep == 0:\n",
    "        return np.random.choice(len(value_estimates))\n",
    "    \n",
    "    confidence_bonuses = confidence_level * np.sqrt(\n",
    "        np.log(timestep) / (pull_counts + 1e-10)\n",
    "    )\n",
    "    ucb_values = value_estimates + confidence_bonuses\n",
    "    return np.argmax(ucb_values)\n",
    "\n",
    "def compute_softmax_probabilities(preference_values):\n",
    "    \"\"\"\n",
    "    converts preference values to action probabilities using softmax\n",
    "    numerically stable implementation\n",
    "    \"\"\"\n",
    "    max_pref = np.max(preference_values)\n",
    "    exp_prefs = np.exp(preference_values - max_pref)\n",
    "    return exp_prefs / np.sum(exp_prefs)\n",
    "\n",
    "def select_gradient_bandit_arm(preference_values):\n",
    "    \"\"\"\n",
    "    chooses arm using gradient bandit with softmax action probabilities\n",
    "    returns both the chosen arm and the probability distribution\n",
    "    \"\"\"\n",
    "    action_probabilities = compute_softmax_probabilities(preference_values)\n",
    "    chosen_arm = np.random.choice(len(preference_values), p=action_probabilities)\n",
    "    return chosen_arm, action_probabilities\n",
    "\n",
    "def update_value_estimate_sample_average(current_estimate, new_reward, pull_count):\n",
    "    \"\"\"\n",
    "    updates value estimate using sample averaging\n",
    "    maintains exact average of all rewards received for this arm\n",
    "    \"\"\"\n",
    "    return current_estimate + (new_reward - current_estimate) / pull_count\n",
    "\n",
    "def update_value_estimate_constant_step(current_estimate, new_reward, step_size):\n",
    "    \"\"\"\n",
    "    updates value estimate using constant step size\n",
    "    gives more weight to recent rewards (exponential recency weighting)\n",
    "    \"\"\"\n",
    "    return current_estimate + step_size * (new_reward - current_estimate)\n",
    "\n",
    "def update_gradient_preferences(preferences, chosen_arm, reward, baseline_reward, \n",
    "                              action_probs, step_size):\n",
    "    \"\"\"\n",
    "    updates all preference values using policy gradient rule\n",
    "    increases preference for chosen arm if reward above baseline\n",
    "    \"\"\"\n",
    "    reward_advantage = reward - baseline_reward\n",
    "    \n",
    "    for arm_idx in range(len(preferences)):\n",
    "        if arm_idx == chosen_arm:\n",
    "            # increase preference for chosen arm based on advantage\n",
    "            preferences[arm_idx] += step_size * reward_advantage * (1 - action_probs[arm_idx])\n",
    "        else:\n",
    "            # decrease preferences for non-chosen arms  \n",
    "            preferences[arm_idx] -= step_size * reward_advantage * action_probs[arm_idx]\n",
    "    \n",
    "    return preferences\n",
    "\n",
    "def run_single_parameter_experiment(strategy_type, param_value, total_runs=2000, \n",
    "                                   max_steps=1000, arm_count=10):\n",
    "    \"\"\"\n",
    "    runs experiment for one parameter value across multiple independent runs\n",
    "    returns average reward across all runs and steps\n",
    "    \"\"\"\n",
    "    cumulative_rewards = 0\n",
    "    total_steps_all_runs = total_runs * max_steps\n",
    "    \n",
    "    for run_number in range(total_runs):\n",
    "        # create fresh bandit problem for this run\n",
    "        true_rewards = setup_bandit_environment(arm_count)\n",
    "        \n",
    "        # initialize variables based on strategy type\n",
    "        if strategy_type == 'optimistic_greedy':\n",
    "            value_estimates = np.full(arm_count, param_value)  # param_value is initial Q\n",
    "            learning_rate = 0.1  # fixed for optimistic initialization\n",
    "        else:\n",
    "            value_estimates = np.zeros(arm_count)\n",
    "            \n",
    "        pull_counts = np.zeros(arm_count)\n",
    "        \n",
    "        # gradient bandit specific initialization\n",
    "        if strategy_type == 'gradient_bandit':\n",
    "            preference_values = np.zeros(arm_count)\n",
    "            running_reward_average = 0\n",
    "            \n",
    "        # run single experiment for max_steps\n",
    "        for step in range(max_steps):\n",
    "            # select arm based on strategy\n",
    "            if strategy_type == 'epsilon_greedy':\n",
    "                chosen_arm = select_epsilon_greedy_arm(value_estimates, param_value)\n",
    "                \n",
    "            elif strategy_type == 'ucb':\n",
    "                chosen_arm = select_ucb_arm(value_estimates, pull_counts, step, param_value)\n",
    "                \n",
    "            elif strategy_type == 'gradient_bandit':\n",
    "                chosen_arm, action_probabilities = select_gradient_bandit_arm(preference_values)\n",
    "                \n",
    "            elif strategy_type == 'optimistic_greedy':\n",
    "                chosen_arm = select_epsilon_greedy_arm(value_estimates, 0.0)  # pure greedy\n",
    "                \n",
    "            # get reward and update counts\n",
    "            step_reward = get_noisy_reward(chosen_arm, true_rewards)\n",
    "            pull_counts[chosen_arm] += 1\n",
    "            cumulative_rewards += step_reward\n",
    "            \n",
    "            # update estimates based on strategy\n",
    "            if strategy_type in ['epsilon_greedy', 'ucb']:\n",
    "                # use sample averaging for these strategies\n",
    "                value_estimates[chosen_arm] = update_value_estimate_sample_average(\n",
    "                    value_estimates[chosen_arm], step_reward, pull_counts[chosen_arm]\n",
    "                )\n",
    "                \n",
    "            elif strategy_type == 'optimistic_greedy':\n",
    "                # use constant step size for optimistic initialization\n",
    "                value_estimates[chosen_arm] = update_value_estimate_constant_step(\n",
    "                    value_estimates[chosen_arm], step_reward, learning_rate\n",
    "                )\n",
    "                \n",
    "            elif strategy_type == 'gradient_bandit':\n",
    "                # update running average reward baseline\n",
    "                running_reward_average += (step_reward - running_reward_average) / (step + 1)\n",
    "                \n",
    "                # update preferences using policy gradient\n",
    "                preference_values = update_gradient_preferences(\n",
    "                    preference_values, chosen_arm, step_reward, running_reward_average,\n",
    "                    action_probabilities, param_value  # param_value is step_size here\n",
    "                )\n",
    "    \n",
    "    # return average reward per step across all runs\n",
    "    return cumulative_rewards / total_steps_all_runs\n",
    "\n",
    "def conduct_parameter_sweep_study(experiment_runs=1000, experiment_steps=1000):\n",
    "    \"\"\"\n",
    "    recreates sutton & barto figure 2.6 comparing bandit algorithm parameter sensitivity\n",
    "    tests each algorithm across range of parameter values using powers of 2\n",
    "    \"\"\"\n",
    "    \n",
    "    # algorithm configuration\n",
    "    strategy_names = [\n",
    "        'epsilon-greedy',\n",
    "        'gradient bandit', \n",
    "        'upper confidence bound',\n",
    "        'greedy with optimistic init'\n",
    "    ]\n",
    "    \n",
    "    strategy_types = [\n",
    "        'epsilon_greedy',\n",
    "        'gradient_bandit', \n",
    "        'ucb',\n",
    "        'optimistic_greedy'\n",
    "    ]\n",
    "    \n",
    "    # parameter ranges as powers of 2\n",
    "    param_power_ranges = [\n",
    "        np.arange(-7, -1, dtype=np.float64),  # epsilon: 2^-7 to 2^-2\n",
    "        np.arange(-5, 2, dtype=np.float64),   # alpha: 2^-5 to 2^1\n",
    "        np.arange(-4, 3, dtype=np.float64),   # c: 2^-4 to 2^2\n",
    "        np.arange(-2, 3, dtype=np.float64)    # q0: 2^-2 to 2^2\n",
    "    ]\n",
    "    # note: ranges chosen to match figure 2.6 in sutton & barto\n",
    "    \n",
    "    # collect results for plotting\n",
    "    all_algorithm_results = []\n",
    "    all_param_values = []\n",
    "    \n",
    "    for strategy_idx, (strategy_name, strategy_type, power_range) in enumerate(\n",
    "        zip(strategy_names, strategy_types, param_power_ranges)\n",
    "    ):\n",
    "        print(f\"\\ntesting {strategy_name}...\")\n",
    "        \n",
    "        algorithm_performance = []\n",
    "        param_values = []\n",
    "        \n",
    "        for power_exponent in power_range:\n",
    "            # convert power of 2 to actual parameter value\n",
    "            actual_param = pow(2, power_exponent)\n",
    "            param_values.append(power_exponent)  # store exponent for x-axis\n",
    "            \n",
    "            print(f\"  parameter 2^{power_exponent:.1f} = {actual_param:.6f}\")\n",
    "            \n",
    "            # run experiment with this parameter value\n",
    "            avg_performance = run_single_parameter_experiment(\n",
    "                strategy_type, actual_param, experiment_runs, experiment_steps\n",
    "            )\n",
    "            \n",
    "            algorithm_performance.append(avg_performance)\n",
    "            \n",
    "        all_algorithm_results.append(algorithm_performance)\n",
    "        all_param_values.append(param_values)\n",
    "    \n",
    "    # create comparison plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = ['red', 'green', 'blue', 'black']\n",
    "    labels = [r'$\\varepsilon$-greedy', 'gradient bandit', 'UCB', 'greedy (optimistic init)\\n$\\\\alpha=0.1$']\n",
    "\n",
    "    for i, (param_vals, perf, color, label) in enumerate(zip(all_param_values, all_algorithm_results, colors, labels)):\n",
    "        plt.plot(param_vals, perf, color=color, linewidth=2.5, label=label)\n",
    "\n",
    "    plt.xticks(\n",
    "        ticks=[-7, -6, -5, -4, -3, -2, -1, 0, 1, 2],\n",
    "        labels=[f'{int(v)}' for v in [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2]],\n",
    "        fontsize=12\n",
    "    )\n",
    "    plt.xlabel('parameter value ($2^x$)', fontsize=13)\n",
    "    plt.ylabel('average reward over first 1000 steps', fontsize=13)\n",
    "    plt.title('parameter study of multi-armed bandit algorithms', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(1.0, 1.5)\n",
    "    plt.xlim(-7, 2)\n",
    "    plt.grid(True, which='both', color='gray', alpha=0.2, linestyle='--')\n",
    "    plt.legend(loc='upper left', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"multi-armed bandit parameter study - function-based implementation\")\n",
    "    print(\"recreating sutton & barto figure 2.6\")\n",
    "    \n",
    "    # run parameter study with moderate settings for reasonable execution time\n",
    "    conduct_parameter_sweep_study(experiment_runs=800, experiment_steps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
