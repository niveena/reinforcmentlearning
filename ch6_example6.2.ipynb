{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17754a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "example 6.2: random walk - td(0) vs monte carlo\n",
      "============================================================\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Niveen Abdul-Mohsen (bvn9ad)\n",
    "# Reinforcement Learning (CS 4771) - Example 6.2\n",
    "# Random Walk Markov Reward Problem\n",
    "# i used numpy for numerical operations and matplotlib for plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class RandomWalkEnvironment:\n",
    "    \"\"\"\n",
    "    random walk markov reward process\n",
    "    \n",
    "    from sutton & barto section 6.2:\n",
    "    \"consider a random walk along a linear sequence of states. the walk begins\n",
    "    in the center state. at each step, the walk moves either to the left or right\n",
    "    by one state with equal probability. the walk terminates when it reaches either\n",
    "    end of the sequence.\"\n",
    "    \n",
    "    environment structure:\n",
    "    - 5 non-terminal states: A, B, C, D, E\n",
    "    - 2 terminal states: left terminal and right terminal\n",
    "    - agent starts in center state C\n",
    "    - each step: move left or right with probability 0.5\n",
    "    - reward is 0 on all transitions except terminating right (reward = 1)\n",
    "    - discount factor γ = 1 (undiscounted)\n",
    "    \"\"\"\n",
    "    \n",
    "    # state constants\n",
    "    STATE_A = 0\n",
    "    STATE_B = 1\n",
    "    STATE_C = 2  # starting state\n",
    "    STATE_D = 3\n",
    "    STATE_E = 4\n",
    "    \n",
    "    # terminal markers\n",
    "    TERMINAL_LEFT = -1\n",
    "    TERMINAL_RIGHT = 5\n",
    "    \n",
    "    # starting position\n",
    "    START_STATE = STATE_C\n",
    "    \n",
    "    # reward values\n",
    "    REWARD_TERMINATE_RIGHT = 1.0\n",
    "    REWARD_DEFAULT = 0.0\n",
    "    \n",
    "    def __init__(self, seed=None):\n",
    "        \"\"\"\n",
    "        initialize random walk environment\n",
    "        \n",
    "        args:\n",
    "            seed: random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.current_state = None\n",
    "        \n",
    "        # state names for printing/debugging\n",
    "        self.state_names = {\n",
    "            self.STATE_A: 'A',\n",
    "            self.STATE_B: 'B', \n",
    "            self.STATE_C: 'C',\n",
    "            self.STATE_D: 'D',\n",
    "            self.STATE_E: 'E',\n",
    "            self.TERMINAL_LEFT: 'LEFT',\n",
    "            self.TERMINAL_RIGHT: 'RIGHT'\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        start new episode\n",
    "        \n",
    "        from textbook: \"all walks start in the center state\"\n",
    "        \n",
    "        returns:\n",
    "            initial_state: the starting state (always STATE_C)\n",
    "        \"\"\"\n",
    "        self.current_state = self.START_STATE\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        take one random step in the environment\n",
    "        \n",
    "        from textbook section 6.2:\n",
    "        \"at each step, the walk moves to one of the two neighboring states\n",
    "        with equal probability. when either terminal state is reached, the\n",
    "        walk terminates.\"\n",
    "        \n",
    "        returns:\n",
    "            next_state: state after taking the step\n",
    "            reward: reward received (0 except 1 when reaching right terminal)\n",
    "            is_terminal: whether episode ended\n",
    "        \"\"\"\n",
    "        move_right = self.rng.random() < 0.5\n",
    "        \n",
    "        if move_right:\n",
    "            next_state = self.current_state + 1\n",
    "        else:\n",
    "            next_state = self.current_state - 1\n",
    "        \n",
    "        # check if we reached a terminal state\n",
    "        if next_state == self.TERMINAL_LEFT:\n",
    "            # terminated on left - reward is 0\n",
    "            reward = self.REWARD_DEFAULT\n",
    "            is_terminal = True\n",
    "        elif next_state == self.TERMINAL_RIGHT:\n",
    "            # terminated on right - reward is 1\n",
    "            reward = self.REWARD_TERMINATE_RIGHT\n",
    "            is_terminal = True\n",
    "        else:\n",
    "            # still in non-terminal state - reward is 0\n",
    "            reward = self.REWARD_DEFAULT\n",
    "            is_terminal = False\n",
    "            self.current_state = next_state\n",
    "        \n",
    "        return next_state, reward, is_terminal\n",
    "    \n",
    "    def get_true_values(self):\n",
    "        \"\"\"\n",
    "        return analytically computed true state values\n",
    "        \n",
    "        from textbook: \"the true value of each state under the random walk\n",
    "        is the probability of terminating on the right if started from that state.\"\n",
    "        \n",
    "        these can be computed as:\n",
    "        V(A) = 1/6, V(B) = 2/6, V(C) = 3/6, V(D) = 4/6, V(E) = 5/6\n",
    "        \"\"\"\n",
    "        true_values = {\n",
    "            self.STATE_A: 1.0/6.0,\n",
    "            self.STATE_B: 2.0/6.0,\n",
    "            self.STATE_C: 3.0/6.0,\n",
    "            self.STATE_D: 4.0/6.0,\n",
    "            self.STATE_E: 5.0/6.0\n",
    "        }\n",
    "        return true_values\n",
    "\n",
    "\n",
    "def td_zero_prediction(env, num_episodes, alpha, initial_value=0.5):\n",
    "    \"\"\"\n",
    "    temporal difference td(0) algorithm for value prediction\n",
    "    \n",
    "    from sutton & barto section 6.1:\n",
    "    \"whereas monte carlo methods must wait until the end of the episode to\n",
    "    determine the increment to V(S_t), TD methods need to wait only until\n",
    "    the next time step.\"\n",
    "    \n",
    "    algorithm pseudocode (from page 120):\n",
    "    initialize V(s) arbitrarily for all s, except V(terminal) = 0\n",
    "    \n",
    "    loop for each episode:\n",
    "        initialize S\n",
    "        loop for each step of episode:\n",
    "            take action, observe R, S'\n",
    "            V(S) ← V(S) + α[R + γV(S') - V(S)]\n",
    "            S ← S'\n",
    "        until S is terminal\n",
    "    \n",
    "    the key update rule:\n",
    "        V(S) ← V(S) + α[R + γV(S') - V(S)]\n",
    "    \n",
    "    this is called \"TD(0)\" or \"one-step TD\" because it updates based on\n",
    "    one step of experience (the immediate reward + next state value estimate)\n",
    "    \n",
    "    args:\n",
    "        env: random walk environment instance\n",
    "        num_episodes: number of training episodes to run\n",
    "        alpha: learning rate (step-size parameter)\n",
    "        initial_value: initial value estimate for all non-terminal states\n",
    "    \n",
    "    returns:\n",
    "        value_function: dictionary mapping states to learned value estimates\n",
    "    \"\"\"\n",
    "    # TODO: initialize value function using defaultdict(float)\n",
    "    # set all non-terminal states (STATE_A through STATE_E) to initial_value\n",
    "    # set terminal states to 0.0\n",
    "    \n",
    "    value_function = defaultdict(float)\n",
    "    for state in range(env.STATE_A, env.STATE_E + 1):\n",
    "        value_function[state] = initial_value\n",
    "    \n",
    "    # terminal states have value 0\n",
    "    value_function[env.TERMINAL_LEFT] = 0.0\n",
    "    value_function[env.TERMINAL_RIGHT] = 0.0\n",
    "    \n",
    "    gamma = 1.0  # discount factor (undiscounted episode)\n",
    "    \n",
    "    # run episodes\n",
    "    for episode_idx in range(num_episodes):\n",
    "        # start new episode\n",
    "        current_state = env.reset()\n",
    "        \n",
    "        # run until terminal state\n",
    "        episode_finished = False\n",
    "        while not episode_finished:\n",
    "            # take random step\n",
    "            next_state, reward, episode_finished = env.step()\n",
    "            \n",
    "            # td(0) update rule:\n",
    "            # V(S) ← V(S) + α[R + γV(S') - V(S)]\n",
    "            # this is the core of temporal difference learning\n",
    "            td_target = reward + gamma * value_function[next_state]\n",
    "            td_error = td_target - value_function[current_state]\n",
    "            value_function[current_state] += alpha * td_error\n",
    "            \n",
    "            # move to next state\n",
    "            current_state = next_state\n",
    "    \n",
    "    return value_function\n",
    "\n",
    "\n",
    "\n",
    "def monte_carlo_prediction(env, num_episodes, alpha, initial_value=0.5):\n",
    "    \"\"\"\n",
    "    constant-alpha monte carlo prediction\n",
    "    \n",
    "    from sutton & barto section 6.1:\n",
    "    \"whereas monte carlo methods must wait until the end of the episode to\n",
    "    determine the increment to V(S_t) (only then is G_t known), TD methods\n",
    "    need wait only until the next time step.\"\n",
    "    \n",
    "    monte carlo update rule (equation 6.1, page 120):\n",
    "        V(S_t) ← V(S_t) + α[G_t - V(S_t)]\n",
    "    \n",
    "    where G_t is the actual return (sum of rewards) from time t onward\n",
    "    \n",
    "    key difference from td:\n",
    "    - mc uses actual complete return G_t (must wait for episode end)\n",
    "    - td uses estimated return R + γV(S') (can update immediately)\n",
    "    \n",
    "    args:\n",
    "        env: random walk environment instance\n",
    "        num_episodes: number of training episodes to run\n",
    "        alpha: learning rate (step-size parameter)\n",
    "        initial_value: initial value estimate for all non-terminal states\n",
    "    \n",
    "    returns:\n",
    "        value_function: dictionary mapping states to learned value estimates\n",
    "    \"\"\"\n",
    "    # TODO: initialize value function\n",
    "    # set all non-terminal states to initial_value\n",
    "    # set terminal states to 0.0\n",
    "    value_function = defaultdict(float)\n",
    "    for state in range(env.STATE_A, env.STATE_E + 1):\n",
    "        value_function[state] = initial_value\n",
    "    \n",
    "    value_function[env.TERMINAL_LEFT] = 0.0\n",
    "    value_function[env.TERMINAL_RIGHT] = 0.0\n",
    "    \n",
    "    gamma = 1.0  # discount factor\n",
    "    \n",
    "    # run episodes\n",
    "    for episode_idx in range(num_episodes):\n",
    "        # generate complete episode\n",
    "        episode_states = []\n",
    "        episode_rewards = []\n",
    "        \n",
    "        # start episode\n",
    "        current_state = env.reset()\n",
    "        episode_states.append(current_state)\n",
    "        \n",
    "        # run until terminal\n",
    "        episode_finished = False\n",
    "        while not episode_finished:\n",
    "            next_state, reward, episode_finished = env.step()\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            if not episode_finished:\n",
    "                episode_states.append(next_state)\n",
    "            \n",
    "            current_state = next_state\n",
    "        \n",
    "        # now update values using actual returns\n",
    "        # work backwards from end of episode\n",
    "        cumulative_return = 0.0\n",
    "        \n",
    "        # process each state visited in reverse order\n",
    "        for step_idx in range(len(episode_states) - 1, -1, -1):\n",
    "            state_visited = episode_states[step_idx]\n",
    "            reward_received = episode_rewards[step_idx]\n",
    "            \n",
    "            # calculate return: G = γ*G + R\n",
    "            cumulative_return = gamma * cumulative_return + reward_received\n",
    "            \n",
    "            # monte carlo update: V(S) ← V(S) + α[G - V(S)]\n",
    "            mc_error = cumulative_return - value_function[state_visited]\n",
    "            value_function[state_visited] += alpha * mc_error\n",
    "    \n",
    "    return value_function\n",
    "\n",
    "\n",
    "def compute_rms_error(value_function, true_values):\n",
    "    \"\"\"\n",
    "    compute root mean squared error between estimated and true values\n",
    "    \n",
    "    from textbook section 6.2:\n",
    "    \"to compare the two methods, we measure the root mean-squared error\n",
    "    between the value function learned and the true values\"\n",
    "    \n",
    "    formula: RMSE = sqrt( mean( (V_estimated - V_true)^2 ) )\n",
    "    averaged over all non-terminal states\n",
    "    \n",
    "    args:\n",
    "        value_function: dictionary of estimated state values\n",
    "        true_values: dictionary of true state values\n",
    "    \n",
    "    returns:\n",
    "        rms_error: root mean squared error\n",
    "    \"\"\"\n",
    "    squared_errors = []\n",
    "    for state in true_values.keys():\n",
    "        error = value_function[state] - true_values[state]\n",
    "        squared_errors.append(error ** 2)\n",
    "    \n",
    "    mean_squared_error = np.mean(squared_errors)\n",
    "    rms_error = np.sqrt(mean_squared_error)\n",
    "    \n",
    "    return rms_error\n",
    "\n",
    "\n",
    "def run_single_training_sequence(env, method, num_episodes, alpha):\n",
    "    \"\"\"\n",
    "    run one training sequence and track value function after each episode\n",
    "    \n",
    "    this is used to generate the left plot showing how values evolve\n",
    "    \n",
    "    args:\n",
    "        env: environment instance\n",
    "        method: 'td' or 'mc'\n",
    "        num_episodes: number of episodes to train\n",
    "        alpha: learning rate\n",
    "    \n",
    "    returns:\n",
    "        value_history: list of value function snapshots (one per episode)\n",
    "    \"\"\"\n",
    "    value_function = defaultdict(float)\n",
    "    for state in range(env.STATE_A, env.STATE_E + 1):\n",
    "        value_function[state] = 0.5\n",
    "    value_function[env.TERMINAL_LEFT] = 0.0\n",
    "    value_function[env.TERMINAL_RIGHT] = 0.0\n",
    "    \n",
    "    gamma = 1.0\n",
    "    value_history = []\n",
    "    \n",
    "    for episode_idx in range(num_episodes):\n",
    "        if method == 'td':\n",
    "            # run one td episode\n",
    "            current_state = env.reset()\n",
    "            episode_finished = False\n",
    "            \n",
    "            while not episode_finished:\n",
    "                next_state, reward, episode_finished = env.step()\n",
    "                td_target = reward + gamma * value_function[next_state]\n",
    "                td_error = td_target - value_function[current_state]\n",
    "                value_function[current_state] += alpha * td_error\n",
    "                current_state = next_state\n",
    "        \n",
    "        elif method == 'mc':\n",
    "            # run one mc episode\n",
    "            episode_states = []\n",
    "            episode_rewards = []\n",
    "            current_state = env.reset()\n",
    "            episode_states.append(current_state)\n",
    "            episode_finished = False\n",
    "            \n",
    "            while not episode_finished:\n",
    "                next_state, reward, episode_finished = env.step()\n",
    "                episode_rewards.append(reward)\n",
    "                if not episode_finished:\n",
    "                    episode_states.append(next_state)\n",
    "                current_state = next_state\n",
    "            \n",
    "            # update values\n",
    "            cumulative_return = 0.0\n",
    "            for step_idx in range(len(episode_states) - 1, -1, -1):\n",
    "                state_visited = episode_states[step_idx]\n",
    "                reward_received = episode_rewards[step_idx]\n",
    "                cumulative_return = gamma * cumulative_return + reward_received\n",
    "                mc_error = cumulative_return - value_function[state_visited]\n",
    "                value_function[state_visited] += alpha * mc_error\n",
    "        \n",
    "        # store snapshot of value function\n",
    "        value_snapshot = {\n",
    "            state: value_function[state] \n",
    "            for state in range(env.STATE_A, env.STATE_E + 1)\n",
    "        }\n",
    "        value_history.append(value_snapshot)\n",
    "    \n",
    "    return value_history\n",
    "\n",
    "\n",
    "def run_experiment_for_rms_plot(alpha_values_td, alpha_values_mc, \n",
    "                                num_episodes, num_runs):\n",
    "    \"\"\"\n",
    "    run multiple experiments to generate rms error curves\n",
    "    \n",
    "    from textbook section 6.2 figure 6.2 caption:\n",
    "    \"the data are averaged over 100 runs\"\n",
    "    \n",
    "    this produces the right plot comparing td vs mc performance\n",
    "    \n",
    "    args:\n",
    "        alpha_values_td: list of learning rates to test for td\n",
    "        alpha_values_mc: list of learning rates to test for mc\n",
    "        num_episodes: episodes per run\n",
    "        num_runs: number of independent runs to average over\n",
    "    \n",
    "    returns:\n",
    "        td_errors: dict mapping alpha -> array of rms errors per episode\n",
    "        mc_errors: dict mapping alpha -> array of rms errors per episode\n",
    "    \"\"\"\n",
    "    temp_env = RandomWalkEnvironment()\n",
    "    true_values = temp_env.get_true_values()\n",
    "    \n",
    "    # store results for each alpha\n",
    "    td_errors = {alpha: np.zeros(num_episodes) for alpha in alpha_values_td}\n",
    "    mc_errors = {alpha: np.zeros(num_episodes) for alpha in alpha_values_mc}\n",
    "    \n",
    "    # run multiple independent experiments\n",
    "    for run_idx in range(num_runs):\n",
    "        # use different seed for each run\n",
    "        seed = run_idx\n",
    "        \n",
    "        # test each td alpha value\n",
    "        for alpha in alpha_values_td:\n",
    "            env = RandomWalkEnvironment(seed=seed)\n",
    "            value_history = run_single_training_sequence(\n",
    "                env, method='td', num_episodes=num_episodes, alpha=alpha\n",
    "            )\n",
    "            \n",
    "            # compute rms error after each episode\n",
    "            for episode_idx, value_func in enumerate(value_history):\n",
    "                rms_err = compute_rms_error(value_func, true_values)\n",
    "                td_errors[alpha][episode_idx] += rms_err\n",
    "        \n",
    "        # test each mc alpha value  \n",
    "        for alpha in alpha_values_mc:\n",
    "            env = RandomWalkEnvironment(seed=seed)\n",
    "            value_history = run_single_training_sequence(\n",
    "                env, method='mc', num_episodes=num_episodes, alpha=alpha\n",
    "            )\n",
    "            \n",
    "            for episode_idx, value_func in enumerate(value_history):\n",
    "                rms_err = compute_rms_error(value_func, true_values)\n",
    "                mc_errors[alpha][episode_idx] += rms_err\n",
    "    \n",
    "    # average over all runs\n",
    "    for alpha in alpha_values_td:\n",
    "        td_errors[alpha] /= num_runs\n",
    "    \n",
    "    for alpha in alpha_values_mc:\n",
    "        mc_errors[alpha] /= num_runs\n",
    "    \n",
    "    return td_errors, mc_errors\n",
    "\n",
    "\n",
    "def plot_figure_6_2():\n",
    "    \"\"\"\n",
    "    recreate figure 6.2 from the textbook\n",
    "    \n",
    "    left plot: value estimates at different points during training\n",
    "    right plot: rms error comparison between td and mc\n",
    "    \"\"\"\n",
    "    fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # ===== LEFT PLOT: value estimates over states =====\n",
    "    \n",
    "    # run a single sequence to show value progression\n",
    "    env_demo = RandomWalkEnvironment(seed=42)\n",
    "    \n",
    "    # get true values\n",
    "    true_values = env_demo.get_true_values()\n",
    "    \n",
    "    # run td learning for demo\n",
    "    num_demo_episodes = 100\n",
    "    alpha_demo = 0.1\n",
    "    value_history_td = run_single_training_sequence(\n",
    "        env_demo, method='td', num_episodes=num_demo_episodes, alpha=alpha_demo\n",
    "    )\n",
    "    \n",
    "    # select specific episodes to plot\n",
    "    episodes_to_plot = [0, 1, 10, 100]\n",
    "    \n",
    "    state_names = ['A', 'B', 'C', 'D', 'E']\n",
    "    state_indices = list(range(5))\n",
    "    \n",
    "    # plot true values as horizontal line\n",
    "    true_value_array = [true_values[i] for i in range(5)]\n",
    "    ax_left.plot(state_indices, true_value_array, 'k-', \n",
    "                 linewidth=2, label='True values', marker='o', markersize=6)\n",
    "    \n",
    "    # plot estimated values at different points in training\n",
    "    colors = ['green', 'purple', 'orange', 'blue']\n",
    "    for idx, episode_num in enumerate(episodes_to_plot):\n",
    "        if episode_num == 0:\n",
    "            # initial values (before any learning)\n",
    "            estimated_values = [0.5] * 5\n",
    "            label = f'{episode_num}'\n",
    "        else:\n",
    "            # after episode_num episodes\n",
    "            value_func = value_history_td[episode_num - 1]\n",
    "            estimated_values = [value_func[state] for state in range(5)]\n",
    "            label = f'{episode_num}'\n",
    "        \n",
    "        ax_left.plot(state_indices, estimated_values, \n",
    "                    color=colors[idx], linewidth=1.5, \n",
    "                    label=label, marker='s', markersize=5)\n",
    "    \n",
    "    ax_left.set_xlabel('State', fontsize=12)\n",
    "    ax_left.set_ylabel('Estimated value', fontsize=12)\n",
    "    ax_left.set_xticks(state_indices)\n",
    "    ax_left.set_xticklabels(state_names)\n",
    "    ax_left.set_ylim([0, 1])\n",
    "    ax_left.legend(loc='upper left', fontsize=10, framealpha=0.9)\n",
    "    ax_left.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax_left.set_title('Value estimates during TD learning', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # ===== RIGHT PLOT: rms error comparison =====\n",
    "    \n",
    "    # parameters for rms experiment\n",
    "    alpha_values_td = [0.05, 0.1, 0.15]\n",
    "    alpha_values_mc = [0.01, 0.02, 0.03, 0.04]\n",
    "    num_episodes_rms = 100\n",
    "    num_runs_rms = 100\n",
    "    \n",
    "    print(\"running rms error experiments...\")\n",
    "    print(f\"  episodes per run: {num_episodes_rms}\")\n",
    "    print(f\"  independent runs: {num_runs_rms}\")\n",
    "    \n",
    "    td_errors, mc_errors = run_experiment_for_rms_plot(\n",
    "        alpha_values_td, alpha_values_mc, num_episodes_rms, num_runs_rms\n",
    "    )\n",
    "    \n",
    "    # plot td curves\n",
    "    td_colors = ['blue', 'cyan', 'purple']\n",
    "    for idx, alpha in enumerate(alpha_values_td):\n",
    "        ax_right.plot(range(num_episodes_rms), td_errors[alpha],\n",
    "                     color=td_colors[idx], linewidth=1.5,\n",
    "                     label=f'TD α={alpha}', linestyle='-')\n",
    "    \n",
    "    # plot mc curves  \n",
    "    mc_colors = ['red', 'orange', 'pink', 'brown']\n",
    "    for idx, alpha in enumerate(alpha_values_mc):\n",
    "        ax_right.plot(range(num_episodes_rms), mc_errors[alpha],\n",
    "                     color=mc_colors[idx], linewidth=1.5,\n",
    "                     label=f'MC α={alpha}', linestyle='--')\n",
    "    \n",
    "    ax_right.set_xlabel('Walks / Episodes', fontsize=12)\n",
    "    ax_right.set_ylabel('Empirical RMS error,\\naveraged over states', fontsize=12)\n",
    "    ax_right.set_xlim([0, num_episodes_rms])\n",
    "    ax_right.set_ylim([0, 0.25])\n",
    "    ax_right.legend(loc='upper right', fontsize=9, framealpha=0.9)\n",
    "    ax_right.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax_right.set_title('TD vs MC Performance Comparison', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # add text labels on plot\n",
    "    ax_right.text(70, 0.06, 'TD', fontsize=14, fontweight='bold', color='blue')\n",
    "    ax_right.text(70, 0.15, 'MC', fontsize=14, fontweight='bold', color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure_6_2_random_walk.png', dpi=300, bbox_inches='tight', \n",
    "                facecolor='white')\n",
    "    print(\"\\nfigure saved as 'figure_6_2_random_walk.png'\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"example 6.2: random walk - td(0) vs monte carlo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # create environment and verify true values\n",
    "    env = RandomWalkEnvironment()\n",
    "    true_vals = env.get_true_values()\n",
    "    \n",
    "    print(\"\\ntrue state values:\")\n",
    "    for state in range(5):\n",
    "        state_name = env.state_names[state]\n",
    "        print(f\"  V({state_name}) = {true_vals[state]:.4f}\")\n",
    "    \n",
    "    print(\"\\ngenerating figure 6.2...\")\n",
    "    plot_figure_6_2()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
