{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17754a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "example 6.2: random walk - td(0) vs monte carlo\n",
      "============================================================\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Niveen Abdul-Mohsen (bvn9ad)\n",
    "# Reinforcement Learning (CS 4771) - Example 6.2\n",
    "# Random Walk Markov Reward Problem\n",
    "# i used numpy for numerical operations and matplotlib for plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class RandomWalkEnvironment:\n",
    "    \"\"\"\n",
    "    random walk markov reward process\n",
    "    \n",
    "    from sutton & barto section 6.2:\n",
    "    \"consider a random walk along a linear sequence of states. the walk begins\n",
    "    in the center state. at each step, the walk moves either to the left or right\n",
    "    by one state with equal probability. the walk terminates when it reaches either\n",
    "    end of the sequence.\"\n",
    "    \n",
    "    environment structure:\n",
    "    - 5 non-terminal states: A, B, C, D, E\n",
    "    - 2 terminal states: left terminal and right terminal\n",
    "    - agent starts in center state C\n",
    "    - each step: move left or right with probability 0.5\n",
    "    - reward is 0 on all transitions except terminating right (reward = 1)\n",
    "    - discount factor γ = 1 (undiscounted)\n",
    "    \"\"\"\n",
    "    \n",
    "    # state constants\n",
    "    STATE_A = 0\n",
    "    STATE_B = 1\n",
    "    STATE_C = 2  # starting state\n",
    "    STATE_D = 3\n",
    "    STATE_E = 4\n",
    "    \n",
    "    # terminal markers\n",
    "    TERMINAL_LEFT = -1\n",
    "    TERMINAL_RIGHT = 5\n",
    "    \n",
    "    # starting position\n",
    "    START_STATE = STATE_C\n",
    "    \n",
    "    # reward values\n",
    "    REWARD_TERMINATE_RIGHT = 1.0\n",
    "    REWARD_DEFAULT = 0.0\n",
    "    \n",
    "    def __init__(self, seed=None):\n",
    "        \"\"\"\n",
    "        initialize random walk environment\n",
    "        \n",
    "        args:\n",
    "            seed: random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.current_state = None\n",
    "        \n",
    "        # state names for printing/debugging\n",
    "        self.state_names = {\n",
    "            self.STATE_A: 'A',\n",
    "            self.STATE_B: 'B', \n",
    "            self.STATE_C: 'C',\n",
    "            self.STATE_D: 'D',\n",
    "            self.STATE_E: 'E',\n",
    "            self.TERMINAL_LEFT: 'LEFT',\n",
    "            self.TERMINAL_RIGHT: 'RIGHT'\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        start new episode\n",
    "        \n",
    "        from textbook: \"all walks start in the center state\"\n",
    "        \n",
    "        returns:\n",
    "            initial_state: the starting state (always STATE_C)\n",
    "        \"\"\"\n",
    "        self.current_state = self.START_STATE\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        take one random step in the environment\n",
    "        \n",
    "        from textbook section 6.2:\n",
    "        \"at each step, the walk moves to one of the two neighboring states\n",
    "        with equal probability. when either terminal state is reached, the\n",
    "        walk terminates.\"\n",
    "        \n",
    "        returns:\n",
    "            next_state: state after taking the step\n",
    "            reward: reward received (0 except 1 when reaching right terminal)\n",
    "            is_terminal: whether episode ended\n",
    "        \"\"\"\n",
    "        move_right = self.rng.random() < 0.5\n",
    "        \n",
    "        if move_right:\n",
    "            next_state = self.current_state + 1\n",
    "        else:\n",
    "            next_state = self.current_state - 1\n",
    "        \n",
    "        # check if we reached a terminal state\n",
    "        if next_state == self.TERMINAL_LEFT:\n",
    "            # terminated on left - reward is 0\n",
    "            reward = self.REWARD_DEFAULT\n",
    "            is_terminal = True\n",
    "        elif next_state == self.TERMINAL_RIGHT:\n",
    "            # terminated on right - reward is 1\n",
    "            reward = self.REWARD_TERMINATE_RIGHT\n",
    "            is_terminal = True\n",
    "        else:\n",
    "            # still in non-terminal state - reward is 0\n",
    "            reward = self.REWARD_DEFAULT\n",
    "            is_terminal = False\n",
    "            self.current_state = next_state\n",
    "        \n",
    "        return next_state, reward, is_terminal\n",
    "    \n",
    "    def get_true_values(self):\n",
    "        \"\"\"\n",
    "        return analytically computed true state values\n",
    "        \n",
    "        from textbook: \"the true value of each state under the random walk\n",
    "        is the probability of terminating on the right if started from that state.\"\n",
    "        \n",
    "        these can be computed as:\n",
    "        V(A) = 1/6, V(B) = 2/6, V(C) = 3/6, V(D) = 4/6, V(E) = 5/6\n",
    "        \"\"\"\n",
    "        true_values = {\n",
    "            self.STATE_A: 1.0/6.0,\n",
    "            self.STATE_B: 2.0/6.0,\n",
    "            self.STATE_C: 3.0/6.0,\n",
    "            self.STATE_D: 4.0/6.0,\n",
    "            self.STATE_E: 5.0/6.0\n",
    "        }\n",
    "        return true_values\n",
    "\n",
    "\n",
    "def td_zero_prediction(env, num_episodes, alpha, initial_value=0.5):\n",
    "    \"\"\"\n",
    "    temporal difference td(0) algorithm for value prediction\n",
    "    \n",
    "    from sutton & barto section 6.1:\n",
    "    \"whereas monte carlo methods must wait until the end of the episode to\n",
    "    determine the increment to V(S_t), TD methods need to wait only until\n",
    "    the next time step.\"\n",
    "    \n",
    "    algorithm pseudocode (from page 120):\n",
    "    initialize V(s) arbitrarily for all s, except V(terminal) = 0\n",
    "    \n",
    "    loop for each episode:\n",
    "        initialize S\n",
    "        loop for each step of episode:\n",
    "            take action, observe R, S'\n",
    "            V(S) ← V(S) + α[R + γV(S') - V(S)]\n",
    "            S ← S'\n",
    "        until S is terminal\n",
    "    \n",
    "    the key update rule:\n",
    "        V(S) ← V(S) + α[R + γV(S') - V(S)]\n",
    "    \n",
    "    this is called \"TD(0)\" or \"one-step TD\" because it updates based on\n",
    "    one step of experience (the immediate reward + next state value estimate)\n",
    "    \n",
    "    args:\n",
    "        env: random walk environment instance\n",
    "        num_episodes: number of training episodes to run\n",
    "        alpha: learning rate (step-size parameter)\n",
    "        initial_value: initial value estimate for all non-terminal states\n",
    "    \n",
    "    returns:\n",
    "        value_function: dictionary mapping states to learned value estimates\n",
    "    \"\"\"\n",
    "    # TODO: initialize value function using defaultdict(float)\n",
    "    # set all non-terminal states (STATE_A through STATE_E) to initial_value\n",
    "    # set terminal states to 0.0\n",
    "    \n",
    "    value_function = defaultdict(float)\n",
    "    for state in range(env.STATE_A, env.STATE_E + 1):\n",
    "        value_function[state] = initial_value\n",
    "    \n",
    "    # terminal states have value 0\n",
    "    value_function[env.TERMINAL_LEFT] = 0.0\n",
    "    value_function[env.TERMINAL_RIGHT] = 0.0\n",
    "    \n",
    "    gamma = 1.0  # discount factor (undiscounted episode)\n",
    "    \n",
    "    # run episodes\n",
    "    for episode_idx in range(num_episodes):\n",
    "        # start new episode\n",
    "        current_state = env.reset()\n",
    "        \n",
    "        # run until terminal state\n",
    "        episode_finished = False\n",
    "        while not episode_finished:\n",
    "            # take random step\n",
    "            next_state, reward, episode_finished = env.step()\n",
    "            \n",
    "            # td(0) update rule:\n",
    "            # V(S) ← V(S) + α[R + γV(S') - V(S)]\n",
    "            # this is the core of temporal difference learning\n",
    "            td_target = reward + gamma * value_function[next_state]\n",
    "            td_error = td_target - value_function[current_state]\n",
    "            value_function[current_state] += alpha * td_error\n",
    "            \n",
    "            # move to next state\n",
    "            current_state = next_state\n",
    "    \n",
    "    return value_function\n",
    "\n",
    "\n",
    "\n",
    "def monte_carlo_prediction(env, num_episodes, alpha, initial_value=0.5):\n",
    "    \"\"\"\n",
    "    constant-alpha monte carlo prediction\n",
    "    \n",
    "    from sutton & barto section 6.1:\n",
    "    \"whereas monte carlo methods must wait until the end of the episode to\n",
    "    determine the increment to V(S_t) (only then is G_t known), TD methods\n",
    "    need wait only until the next time step.\"\n",
    "    \n",
    "    monte carlo update rule (equation 6.1, page 120):\n",
    "        V(S_t) ← V(S_t) + α[G_t - V(S_t)]\n",
    "    \n",
    "    where G_t is the actual return (sum of rewards) from time t onward\n",
    "    \n",
    "    key difference from td:\n",
    "    - mc uses actual complete return G_t (must wait for episode end)\n",
    "    - td uses estimated return R + γV(S') (can update immediately)\n",
    "    \n",
    "    args:\n",
    "        env: random walk environment instance\n",
    "        num_episodes: number of training episodes to run\n",
    "        alpha: learning rate (step-size parameter)\n",
    "        initial_value: initial value estimate for all non-terminal states\n",
    "    \n",
    "    returns:\n",
    "        value_function: dictionary mapping states to learned value estimates\n",
    "    \"\"\"\n",
    "    # TODO: initialize value function\n",
    "    # set all non-terminal states to initial_value\n",
    "    # set terminal states to 0.0\n",
    "    \n",
    "    gamma = 1.0  # discount factor\n",
    "    \n",
    "    # TODO: loop for num_episodes\n",
    "    #   - create empty lists: episode_states = [], episode_rewards = []\n",
    "    #   \n",
    "    #   - reset environment and append starting state to episode_states\n",
    "    #   \n",
    "    #   - generate complete episode by stepping until terminal:\n",
    "    #       - get next_state, reward, episode_finished from env.step()\n",
    "    #       - append reward to episode_rewards\n",
    "    #       - if not episode_finished, append next_state to episode_states\n",
    "    #   \n",
    "    #   - now episode is complete, calculate returns and update values\n",
    "    #   - initialize cumulative_return = 0.0\n",
    "    #   \n",
    "    #   - loop backwards through episode (from last state to first):\n",
    "    #       - for step_idx from len(episode_states)-1 down to 0:\n",
    "    #           - get state_visited = episode_states[step_idx]\n",
    "    #           - get reward_received = episode_rewards[step_idx]\n",
    "    #           \n",
    "    #           - update return: cumulative_return = gamma * cumulative_return + reward_received\n",
    "    #           \n",
    "    #           - compute mc_error = cumulative_return - value_function[state_visited]\n",
    "    #           \n",
    "    #           - update value: value_function[state_visited] += alpha * mc_error\n",
    "    \n",
    "    # TODO: return value_function\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_rms_error(value_function, true_values):\n",
    "    \"\"\"\n",
    "    compute root mean squared error between estimated and true values\n",
    "    \n",
    "    from textbook section 6.2:\n",
    "    \"to compare the two methods, we measure the root mean-squared error\n",
    "    between the value function learned and the true values\"\n",
    "    \n",
    "    formula: RMSE = sqrt( mean( (V_estimated - V_true)^2 ) )\n",
    "    averaged over all non-terminal states\n",
    "    \n",
    "    args:\n",
    "        value_function: dictionary of estimated state values\n",
    "        true_values: dictionary of true state values\n",
    "    \n",
    "    returns:\n",
    "        rms_error: root mean squared error\n",
    "    \"\"\"\n",
    "    # TODO: create list to store squared errors\n",
    "    # TODO: for each state in true_values:\n",
    "    #   - compute error = value_function[state] - true_values[state]\n",
    "    #   - append error^2 to list\n",
    "    # TODO: compute mean of squared errors\n",
    "    # TODO: return square root of mean\n",
    "    pass\n",
    "\n",
    "\n",
    "def run_single_training_sequence(env, method, num_episodes, alpha):\n",
    "    \"\"\"\n",
    "    run one training sequence and track value function after each episode\n",
    "    \n",
    "    this is used to generate the left plot showing how values evolve\n",
    "    \n",
    "    args:\n",
    "        env: environment instance\n",
    "        method: 'td' or 'mc'\n",
    "        num_episodes: number of episodes to train\n",
    "        alpha: learning rate\n",
    "    \n",
    "    returns:\n",
    "        value_history: list of value function snapshots (one per episode)\n",
    "    \"\"\"\n",
    "    # TODO: initialize value function (all states start at 0.5)\n",
    "    \n",
    "    # TODO: initialize empty list: value_history = []\n",
    "    \n",
    "    # TODO: loop for num_episodes:\n",
    "    #   - if method == 'td':\n",
    "    #       run one td episode (similar to td_zero_prediction but just 1 episode)\n",
    "    #   - elif method == 'mc':\n",
    "    #       run one mc episode (similar to monte_carlo_prediction but just 1 episode)\n",
    "    #   \n",
    "    #   - after each episode, create snapshot of current value function\n",
    "    #   - append snapshot to value_history\n",
    "    \n",
    "    # TODO: return value_history\n",
    "    pass\n",
    "\n",
    "\n",
    "def run_experiment_for_rms_plot(alpha_values_td, alpha_values_mc, \n",
    "                                num_episodes, num_runs):\n",
    "    \"\"\"\n",
    "    run multiple experiments to generate rms error curves\n",
    "    \n",
    "    from textbook section 6.2 figure 6.2 caption:\n",
    "    \"the data are averaged over 100 runs\"\n",
    "    \n",
    "    this produces the right plot comparing td vs mc performance\n",
    "    \n",
    "    args:\n",
    "        alpha_values_td: list of learning rates to test for td\n",
    "        alpha_values_mc: list of learning rates to test for mc\n",
    "        num_episodes: episodes per run\n",
    "        num_runs: number of independent runs to average over\n",
    "    \n",
    "    returns:\n",
    "        td_errors: dict mapping alpha -> array of rms errors per episode\n",
    "        mc_errors: dict mapping alpha -> array of rms errors per episode\n",
    "    \"\"\"\n",
    "    # TODO: get true values from environment\n",
    "    \n",
    "    # TODO: initialize dictionaries to store errors:\n",
    "    #   td_errors[alpha] = np.zeros(num_episodes) for each alpha\n",
    "    #   mc_errors[alpha] = np.zeros(num_episodes) for each alpha\n",
    "    \n",
    "    # TODO: loop for num_runs:\n",
    "    #   use different seed for each run\n",
    "    #   \n",
    "    #   - for each td alpha value:\n",
    "    #       - create environment with seed\n",
    "    #       - run training sequence\n",
    "    #       - for each episode, compute rms error and add to td_errors[alpha]\n",
    "    #   \n",
    "    #   - for each mc alpha value:\n",
    "    #       - create environment with seed\n",
    "    #       - run training sequence  \n",
    "    #       - for each episode, compute rms error and add to mc_errors[alpha]\n",
    "    \n",
    "    # TODO: divide all errors by num_runs to get average\n",
    "    \n",
    "    # TODO: return td_errors, mc_errors\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_figure_6_2():\n",
    "    \"\"\"\n",
    "    recreate figure 6.2 from the textbook\n",
    "    \n",
    "    left plot: value estimates at different points during training\n",
    "    right plot: rms error comparison between td and mc\n",
    "    \"\"\"\n",
    "    fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # ===== LEFT PLOT: value estimates over states =====\n",
    "    \n",
    "    # TODO: create environment, get true values\n",
    "    \n",
    "    # TODO: run training sequence for td (e.g., 100 episodes, alpha=0.1)\n",
    "    \n",
    "    # TODO: select episodes to plot (e.g., [0, 1, 10, 100])\n",
    "    \n",
    "    # TODO: plot true values as black line with markers\n",
    "    \n",
    "    # TODO: for each selected episode:\n",
    "    #   plot estimated values as colored line\n",
    "    \n",
    "    # TODO: set labels, title, legend, grid\n",
    "    \n",
    "    # ===== RIGHT PLOT: rms error comparison =====\n",
    "    \n",
    "    # TODO: define alpha values to test\n",
    "    #   e.g., alpha_values_td = [0.05, 0.1, 0.15]\n",
    "    #        alpha_values_mc = [0.01, 0.02, 0.03, 0.04]\n",
    "    \n",
    "    # TODO: run rms experiments (e.g., 100 episodes, 100 runs)\n",
    "    \n",
    "    # TODO: plot td curves (solid lines)\n",
    "    \n",
    "    # TODO: plot mc curves (dashed lines)\n",
    "    \n",
    "    # TODO: set labels, title, legend, grid\n",
    "    \n",
    "    # TODO: add text labels \"TD\" and \"MC\" on plot\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure_6_2_random_walk.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"example 6.2: random walk - td(0) vs monte carlo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # TODO: create environment\n",
    "    \n",
    "    # TODO: get and print true values\n",
    "    \n",
    "    # TODO: call plot_figure_6_2()\n",
    "    \n",
    "    print(\"\\ndone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
