{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0d7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Example 6.5: Windy Gridworld - SARSA\n",
      "============================================================\n",
      "\n",
      "Parameters:\n",
      "  Episodes: 170\n",
      "  Alpha (step size): 0.5\n",
      "  Gamma (discount): 1.0\n",
      "  Epsilon (exploration): 0.1\n",
      "\n",
      "Running SARSA...\n",
      "Training complete!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 288\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m#print(f\"Total time steps: {time_steps[-1]}\")\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage episode length (last 10): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(episode_lengths[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# TODO: get and display the greedy path\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Niveen Abdul-Mohsen (bvn9ad)\n",
    "# Reinforcement Learning (CS 4771) - Example 6.5\n",
    "# Implementation of SARSA (on-policy TD control) for the Windy Gridworld problem\n",
    "# i used numpy for numerical operations and matplotlib for plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class WindyGridworld:\n",
    "    \"\"\"    \n",
    "\tstandard gridworld with start and goal positions, but with a crosswind\n",
    "    running upward through the middle of the grid. actions are standard\n",
    "    (up, down, right, left) but in some columns the resulting next state\n",
    "    is shifted upward by the wind.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # grid dimensions\n",
    "        self.grid_height = 7\n",
    "        self.grid_width = 10\n",
    "        \n",
    "        # start and goal positions (row, col)\n",
    "        self.start_pos = (3, 0)  # start at middle left\n",
    "        self.goal_pos = (3, 7)   # goal at middle right\n",
    "        \n",
    "        # wind strength for each column (pushes upward)\n",
    "        # columns: 0  1  2  3  4  5  6  7  8  9\n",
    "        self.wind = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
    "        \n",
    "        # actions: up, down, right, left\n",
    "        self.actions = ['up', 'down', 'right', 'left']\n",
    "        self.action_effects = {\n",
    "            'up': (-1, 0),\n",
    "            'down': (1, 0),\n",
    "            'right': (0, 1),\n",
    "            'left': (0, -1)\n",
    "        }\n",
    "        \n",
    "        # current state\n",
    "        self.current_state = None\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"reset environment to start state and return it\"\"\"\n",
    "        self.current_state = self.start_pos\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        take action and return next state, reward, done flag\n",
    "        \n",
    "        process:\n",
    "        1. get current position (row, col)\n",
    "        2. apply the action's effect to get new position\n",
    "        3. apply wind effect from the CURRENT column (before moving)\n",
    "        4. clamp position to be within grid bounds\n",
    "        5. update current_state\n",
    "        6. check if we reached the goal\n",
    "        7. return (next_state, reward, done)\n",
    "        \n",
    "        note: Reward is -1 for each step, regardless of action\n",
    "        \"\"\"\n",
    "        row, col = self.current_state\n",
    "        \n",
    "        # apply action\n",
    "        d_row, d_col = self.action_effects[action]\n",
    "        new_row = row + d_row\n",
    "        new_col = col + d_col\n",
    "        \n",
    "        # apply wind (pushes upward, so subtract from row)\n",
    "        wind_strength = self.wind[col]\n",
    "        new_row -= wind_strength\n",
    "        \n",
    "        # keep within bounds\n",
    "        new_row = max(0, min(self.grid_height - 1, new_row))\n",
    "        new_col = max(0, min(self.grid_width - 1, new_col))\n",
    "        \n",
    "        # update state\n",
    "        self.current_state = (new_row, new_col)\n",
    "        \n",
    "        # check if goal reached\n",
    "        done = (self.current_state == self.goal_pos)\n",
    "        \n",
    "        # reward is -1 until goal is reached\n",
    "        reward = -1\n",
    "        \n",
    "        return self.current_state, reward, done\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"check if state is terminal (goal state)\"\"\"\n",
    "        # TODO: return True if state equals goal_pos, False otherwise\n",
    "        pass\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon, actions):\n",
    "    \"\"\"\n",
    "    choose action using epsilon-greedy policy derived from Q values\n",
    "    \n",
    "    epsilon-greedy: \n",
    "    - with prob epsilon: choose random action (exploration)\n",
    "    - with prob 1-epsilon: choose action with highest Q value (exploitation)\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        # explore: random action\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        # exploit: best action\n",
    "        # get Q values for all actions from this state\n",
    "        q_values = [Q[(state, action)] for action in actions]\n",
    "        max_q = max(q_values)\n",
    "        \n",
    "        # handle ties by randomly choosing among\n",
    "        best_actions = [actions[i] for i in range(len(actions)) \n",
    "                       if q_values[i] == max_q]\n",
    "        return np.random.choice(best_actions)\n",
    "\n",
    "\n",
    "def sarsa(env, num_episodes, alpha=0.5, gamma=1.0, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    SARSA (on-policy TD control) algorithm\n",
    "    \n",
    "    from textbook page 130, the SARSA algorithm:\n",
    "    \n",
    "    initialize Q(s,a) to 0 for all s, a\n",
    "    loop for each episode:\n",
    "        initialize S (reset environment)\n",
    "        choose A from S using policy derived from Q (e.g., epsilon-greedy)\n",
    "        loop for each step of episode:\n",
    "            take action A, observe R, S'\n",
    "            choose A' from S' using policy derived from Q\n",
    "            Q(S,A) <- Q(S,A) + alpha[R + gamma*Q(S',A') - Q(S,A)]\n",
    "            S <- S'; A <- A'\n",
    "        until S is terminal\n",
    "    \n",
    "\tnotes from textbook notes:\n",
    "    - SARSA is ON-POLICY: it learns from the policy it's currently following\n",
    "    - the update uses the ACTUAL next action A' that the policy will take\n",
    "    - this is different from Q-learning, which is OFF-POLICY\n",
    "    \"\"\"\n",
    "    # initialize Q(s,a) to 0 for all state-action pairs\n",
    "    Q = defaultdict(float)\n",
    "    \n",
    "    # track episode lengths for plotting\n",
    "    episode_lengths = []\n",
    "    time_steps = []\n",
    "    total_steps = 0\n",
    "    \n",
    "    # run episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # initialize S\n",
    "        state = env.reset()\n",
    "        \n",
    "        # choose A from S using epsilon-greedy policy derived from Q\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon, env.actions)\n",
    "        \n",
    "        steps_in_episode = 0\n",
    "        \n",
    "        # loop for each step of episode\n",
    "        while True:\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            steps_in_episode += 1\n",
    "            total_steps += 1\n",
    "            \n",
    "            if done:\n",
    "                # terminal state: update Q and end episode\n",
    "                Q[(state, action)] += alpha * (reward - Q[(state, action)])\n",
    "                break\n",
    "            \n",
    "            # choose A' from S' using epsilon-greedy policy derived from Q\n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon, env.actions)\n",
    "            \n",
    "            # SARSA update rule:\n",
    "            # Q(S,A) <- Q(S,A) + alpha[R + gamma*Q(S',A') - Q(S,A)]\n",
    "            td_target = reward + gamma * Q[(next_state, next_action)]\n",
    "            td_error = td_target - Q[(state, action)]\n",
    "            Q[(state, action)] += alpha * td_error\n",
    "            \n",
    "            # S <- S'; A <- A'\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        \n",
    "        episode_lengths.append(steps_in_episode)\n",
    "        time_steps.append(total_steps)\n",
    "    \n",
    "    return Q, episode_lengths, time_steps\n",
    "\n",
    "\n",
    "def plot_results(time_steps, episode_numbers):\n",
    "    \"\"\"\n",
    "    reproduce the plot from example 6.5\n",
    "    \"\"\"\n",
    "    # TODO: create a figure with size (10, 6)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # TODO: plot time_steps vs episode_numbers\n",
    "    \n",
    "    # TODO: add labels and title\n",
    "    \n",
    "    # TODO: add grid for readability\n",
    "    \n",
    "    # TODO: save and display the figure\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def get_greedy_path(env, Q):\n",
    "    \"\"\"\n",
    "    extract the greedy path from start to goal using learned Q values\n",
    "    \n",
    "    this shows what the agent learned: starting from the initial state,\n",
    "    always choose the action with the highest Q value (greedy, no exploration).\n",
    "    \"\"\"\n",
    "    # TODO: reset environment to get starting state\n",
    "    state = None  # TODO: FILL IN\n",
    "    \n",
    "    # TODO: initialize path with starting state\n",
    "    path = None  # TODO: FILL IN\n",
    "    \n",
    "    # TODO: initialize step counter (prevent infinite loops)\n",
    "    steps = 0\n",
    "    max_steps = 100\n",
    "    \n",
    "    # TODO: loop until terminal state or max steps reached\n",
    "    while not env.is_terminal(state) and steps < max_steps:\n",
    "        # TODO: choose greedy action (epsilon=0 means no exploration)\n",
    "        action = None  # TODO: FILL IN\n",
    "        \n",
    "        # TODO: take one step in the environment\n",
    "        state, _, _ = None, None, None  # TODO: FILL IN\n",
    "        \n",
    "        # TODO: ad new state to path\n",
    "        # TODO: iccrement step counter\n",
    "        \n",
    "    # TODO: return the path\n",
    "    return None  # TODO: FILL IN\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Example 6.5: Windy Gridworld - SARSA\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # parameters from the textbook\n",
    "    num_episodes = 170\n",
    "    alpha = 0.5         # step size\n",
    "    gamma = 1.0         # undiscounted (episodic task)\n",
    "    epsilon = 0.1       # exploration rate\n",
    "    \n",
    "    print(f\"Parameters:\")\n",
    "    print(f\"  Episodes: {num_episodes}\")\n",
    "    print(f\"  Alpha (step size): {alpha}\")\n",
    "    print(f\"  Gamma (discount): {gamma}\")\n",
    "    print(f\"  Epsilon (exploration): {epsilon}\")\n",
    "    print()\n",
    "    \n",
    "    # TODO: create environment instance\n",
    "    env = None  # TODO: FILL IN\n",
    "    \n",
    "    print(\"Running SARSA...\")\n",
    "    # TODO: call sarsa() function and unpack results\n",
    "    Q, episode_lengths, time_steps = None, None, None  # TODO: FILL IN\n",
    "    \n",
    "    # TODO: Create list of episode numbers for plotting\n",
    "    episode_numbers = None  # TODO: FILL IN\n",
    "    \n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"Total time steps: {time_steps[-1]}\")\n",
    "    print(f\"Average episode length (last 10): {np.mean(episode_lengths[-10:]):.1f}\")\n",
    "    print()\n",
    "    \n",
    "    # TODO: get and display the greedy path\n",
    "    optimal_path = None  # TODO: FILL IN\n",
    "    print(f\"Greedy policy path length: {len(optimal_path) - 1} steps\")\n",
    "    print(f\"(Book reports minimum of 15 steps)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Generating plot...\")\n",
    "    # TODO: call plot_results() function\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
